{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DataFrames en Spark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n",
    "from pyspark.sql.functions import col, countDistinct, desc, asc\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T13:47:23.646696900Z",
     "start_time": "2023-05-16T13:47:23.574803600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([i for i in range(1000)]).map(lambda x: (x, x ** 2))\n",
    "rdd.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Crear DF a partir de un RDD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Crear un DF a partir de un RDD\n",
    "df = rdd.toDF([\"value\", \"value_squared\"])\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Crear un DF especificando el esquema\n",
    "rdd_names = sc.parallelize([(1, \"John\", 23.4), (2, \"Mary\", 18.3), (3, \"Peter\", 32.8), (4, \"Ann\", 68.5)])\n",
    "\n",
    "# Forma 1\n",
    "\n",
    "schema1 = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"balance\", FloatType(), True),\n",
    "])\n",
    "\n",
    "df1 = spark.createDataFrame(rdd_names, schema=schema1)\n",
    "df1.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Forma2\n",
    "schema2 = \"id INT, name STRING, balance FLOAT\"\n",
    "df2 = spark.createDataFrame(rdd_names, schema=schema2)\n",
    "df2.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Crear DF a partir de diferentes fuentes de datos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Archivo de texto\n",
    "\n",
    "df3 = spark.read.text(paths=r\"../../data/s7_data/dataTXT.txt\")\n",
    "df3.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Archivo CSV\n",
    "\n",
    "# df4 = spark.read.csv(path=r\"../../data/s7_data/dataCSV.csv\", header=True)\n",
    "# O\n",
    "df4 = spark.read.option(\"header\", True).csv(path=r\"../../data/s7_data/dataCSV.csv\")\n",
    "df4.printSchema()\n",
    "df4.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Archivo de texto con delimitador\n",
    "df5 = spark.read.option(\"header\", True).option(\"delimiter\", \"|\").csv(path=r\"../../data/s7_data/dataTab.txt\")\n",
    "df5.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A partir de JSON proporcionando el esquema\n",
    "\n",
    "schema_json = StructType([\n",
    "    StructField(\"pais\", StringType(), False),\n",
    "    StructField(\"edad\", StringType(), True),\n",
    "    StructField(\"fecha\", DateType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "])\n",
    "df6 = spark.read.schema(schema_json).json(path=r\"../../data/s7_data/dataJSON.json\")\n",
    "df6.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A partir de un archivo Parquet\n",
    "\n",
    "# df7 = spark.read.parquet(r\"../../data/s7_data/dataParquet.parquet\")\n",
    "# O\n",
    "df7 = spark.read.format(\"parquet\").load(r\"../../data/s7_data/dataParquet.parquet\")\n",
    "df7.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trabajo con columnas\n",
    "Las operaciones estructuradas están diseñadas para ser más relacionales.\n",
    "\n",
    "Las operaciones estructuradas **NO** son lazy (perezosas), es decir, se ejecutan inmediatamente.\n",
    "\n",
    "Al igual que las operaciones con RDD, las operaciones estructuradas se dividen en dos categorías:\n",
    "\n",
    "* **Transformaciones**: crean un nuevo DF a partir de uno existente.\n",
    "* **Acciones**: devuelven un valor al programa controlador después de realizar un cálculo en el DF."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = spark.read.parquet(r\"../../data/s7_data/dataParquet_2.parquet\")\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Primera alternativa para trabajar con columnas\n",
    "df.select(\"title\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Segunda alternativa para trabajar con columnas\n",
    "df.select(col(\"title\")).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformaciones\n",
    "\n",
    "## Funciones select y selectExpr\n",
    "\n",
    "* **select**: toma como argumentos una o varias columnas del dataframe y devuelve un nuevo dataframe con solo esas columnas seleccionadas.\n",
    "* **selectExpr**: permite seleccionar columnas utilizando una sintaxis similar a la de SQL. Con esta función, podemos seleccionar columnas y realizar operaciones en ellas, como cálculos matemáticos o transformaciones de cadenas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select\n",
    "df = spark.read.parquet(r\"../../data/s7_data/datos.parquet\")\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.select(col(\"video_id\")).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.select(\"video_id\", \"trending_date\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Esto arrojará un error\n",
    "df.select(\n",
    "    \"likes\",\n",
    "    \"dislikes\",\n",
    "    (\"likes\" - \"dislikes\").alias(\"acceptance\"),\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Aquí la ventaja de utilizar la función col\n",
    "df.select(\n",
    "    col(\"likes\"),\n",
    "    col(\"dislikes\"),\n",
    "    (col(\"likes\") - col(\"dislikes\")).alias(\"acceptance\"),\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.select(countDistinct(\"video_id\").alias(\"distinct_videos\")).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Con selectExpr\n",
    "df.selectExpr(\n",
    "    \"likes\",\n",
    "    \"dislikes\",\n",
    "    \"likes - dislikes as acceptance\",\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.selectExpr(\"count(distinct video_id) as distinct_videos\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Funciones filter y where\n",
    "\n",
    "* **filter**: toma como argumento una expresión booleana y devuelve un nuevo dataframe que contiene solo los registros que cumplen con esa expresión.\n",
    "\n",
    "* **where**:  funciona de manera similar a filter(), tomando también una expresión booleana como argumento. La principal diferencia es que where() es una función de conveniencia que se utiliza comúnmente para escribir consultas SQL en PySpark."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.filter(col(\"video_id\") == \"YVfyYrEmzgM\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.where(col(\"video_id\") == \"YVfyYrEmzgM\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.select(col(\"video_id\"), col(\"likes\")).filter(col(\"likes\") >= 5000).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.select(\"video_id\", \"likes\").where(\"likes >= 5000\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.select(\n",
    "    col(\"video_id\"),\n",
    "    col(\"likes\"),\n",
    "    col(\"dislikes\"),\n",
    ").filter(\n",
    "    (col(\"likes\") >= 5000) &\n",
    "    (col(\"dislikes\") >= 100)\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Funciones distinct y dropDuplicates\n",
    "\n",
    "* **distinct**: se utiliza para obtener los valores únicos de una o varias columnas de un dataframe.\n",
    "* **dropDuplicates**: también se utiliza para eliminar los registros duplicados de un dataframe y devolver un nuevo dataframe con solo los registros únicos, pero a diferencia de distinct(), esta función permite especificar las columnas que se deben considerar para determinar si un registro es duplicado o no."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# distinct\n",
    "df_distinct = df.distinct()\n",
    "df_distinct.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dropDuplicates\n",
    "df.dropDuplicates().count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_duplication = spark.createDataFrame([\n",
    "    (1, \"blue\", 100),\n",
    "    (1, \"blue\", 101),\n",
    "    (2, \"red\", 300),\n",
    "    (2, \"red\", 301),\n",
    "    (3, \"green\", 500),\n",
    "    (3, \"green\", 501),\n",
    "    (4, \"yellow\", 700),\n",
    "    (4, \"yellow\", 701),\n",
    "    (5, \"black\", 900),\n",
    "    (5, \"black\", 901),\n",
    "]).toDF(\"id\", \"color\", \"price\")\n",
    "df_duplication.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Al ser el precio diferente, no se considera duplicado\n",
    "df_duplication.dropDuplicates(\n",
    "    subset=[\"id\", \"color\", \"price\"]\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Al solo tener en cuenta el id y el color, se considera duplicado\n",
    "df_duplication.dropDuplicates(\n",
    "    subset=[\"id\", \"color\"]\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Funciones sort y orderBy\n",
    "\n",
    "* **sort**: se utiliza para ordenar los datos de un RDD en función de una o varias columnas.\n",
    "* **orderBy**: es un alias de sort()."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sort\n",
    "df.sort(col(\"likes\")).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# orderBy\n",
    "df.orderBy(col(\"likes\")).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = spark.read.parquet(r\"../../data/s7_data/datos.parquet\").select(\n",
    "    col(\"video_id\"),\n",
    "    col(\"title\"),\n",
    "    col(\"likes\"),\n",
    "    col(\"dislikes\"),\n",
    "    col(\"views\"),\n",
    ").dropDuplicates(subset=[\"video_id\"])\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sort\n",
    "df.sort(desc(col(\"likes\"))).show()\n",
    "# df.sort(col(\"likes\").desc()).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# orderBy\n",
    "df.orderBy(\"likes\", ascending=False).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_duplication = spark.createDataFrame([\n",
    "    (1, \"blue\", 100),\n",
    "    (1, \"blue\", 101),\n",
    "    (2, \"red\", 300),\n",
    "    (2, \"red\", 301),\n",
    "    (3, \"green\", 500),\n",
    "    (3, \"green\", 501),\n",
    "    (4, \"yellow\", 700),\n",
    "    (4, \"yellow\", 701),\n",
    "    (5, \"black\", 900),\n",
    "    (5, \"black\", 901),\n",
    "]).toDF(\"id\", \"color\", \"price\")\n",
    "df_duplication.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_duplication.orderBy(\n",
    "    col(\"color\").asc(),\n",
    "    col(\"price\"),\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Función limit: devuelve un nuevo dataframe con un número determinado de registros.\n",
    "\n",
    "# Top 10 videos con más vistas\n",
    "top_10_views = df.orderBy(\n",
    "    col(\"views\").desc(),\n",
    ").limit(10)\n",
    "top_10_views.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Funciones withColumn y withColumnRenamed\n",
    "\n",
    "* **withColumn**: se utiliza para agregar una nueva columna a un DataFrame existente o para reemplazar una columna existente.\n",
    "* **withColumnRenamed**: es una función que se utiliza para cambiar el nombre de una columna en un DataFrame de PySpark."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = spark.read.parquet(r\"../../data/s7_data/datos.parquet\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# withColumn\n",
    "df.withColumn(\"acceptance\", col(\"likes\") - col(\"dislikes\")).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_df = (\n",
    "    df.\n",
    "    withColumn(\"acceptance\", col(\"likes\") - col(\"dislikes\")).\n",
    "    withColumn(\"likes_percentage_relative_views\", col(\"likes\") / col(\"views\") * 100)\n",
    ")\n",
    "new_df.select(\n",
    "    col(\"video_id\"),\n",
    "    col(\"title\"),\n",
    "    col(\"acceptance\"),\n",
    "    col(\"likes_percentage_relative_views\"),\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# withColumnRenamed\n",
    "df.withColumnRenamed(\"video_id\", \"video_code\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Funciones drop, sample y randomSplit\n",
    "\n",
    "* **drop**: se utiliza para eliminar una o varias columnas de un DataFrame.\n",
    "* **sample**: se utiliza para obtener una muestra aleatoria de un DataFrame.\n",
    "* **randomSplit**:  se utiliza para dividir un DataFrame en dos o más DataFrames de forma aleatoria, según un conjunto de pesos o fracciones."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = spark.read.parquet(r\"../../data/s7_data/datos.parquet\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop\n",
    "clean_df = df.drop(\"tags\", \"thumbnail_link\", \"video_error_or_removed\")\n",
    "clean_df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sample\n",
    "df.sample(fraction=0.1).count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.sample(fraction=0.1, seed=123).count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# withReplacement: indica si se permite que un mismo registro se seleccione más de una vez.\n",
    "df.sample(fraction=0.1, seed=123, withReplacement=False).count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# randomSplit\n",
    "df_1, df_2 = df.randomSplit([0.7, 0.3], seed=123)  # 70% y 30% de los datos respectivamente (aproximadamente)\n",
    "print(f\"original size: {df.count()}\")\n",
    "print(f\"df_1 size: {df_1.count()}\")\n",
    "print(f\"df_2 size: {df_2.count()}\")\n",
    "print(f\"df_1 + df_2 size: {df_1.count() + df_2.count()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trabajo con datos faltantes o incorrectos\n",
    "\n",
    "Las dos formas más habituales de tratar los datos faltantes o incorrectos son:\n",
    "* Eliminar las filas que tienen valores perdidos en una o más columnas.\n",
    "* Llenar los valores perdidos con un valor determinado.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = spark.read.parquet(r\"../../data/s7_data/datos.parquet\")\n",
    "df.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Eliminar las filas.\n",
    "# df.na.drop().count()\n",
    "df.dropna(how=\"any\").count()  # cuando al menos una columna tiene un valor perdido"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.dropna(how=\"all\").count()  # cuando todas las columnas tienen un valor perdido"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"tags\"]).count()  # cuando la columna tags tiene un valor perdido"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Llenar los valores perdidos con un valor determinado.\n",
    "filled_df = df.orderBy(col(\"views\").asc()).select(\n",
    "    col(\"video_id\"),\n",
    "    col(\"title\"),\n",
    "    col(\"views\"),\n",
    "    col(\"likes\"),\n",
    "    col(\"dislikes\"),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Todas las columnas\n",
    "filled_df.fillna(0).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ciertas columnas\n",
    "filled_df.fillna(\n",
    "    value=0,\n",
    "    subset=[\n",
    "        \"views\",\n",
    "        \"likes\",\n",
    "        \"dislikes\",\n",
    "    ]\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Acciones sobre DataFrames en PySpark\n",
    "\n",
    "* **show**: muestra los primeros n registros del DataFrame (20 por defecto).\n",
    "* **take**: devuelve los primeros n registros del DataFrame en forma de lista.\n",
    "* **head**: devuelve los primeros n registros del DataFrame en forma de lista."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = spark.read.parquet(r\"../../data/s7_data/datos.parquet\")\n",
    "\n",
    "# Recordemos que las acciones son operaciones que devuelven un valor, no un DataFrame como en el caso de las transformaciones."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show\n",
    "df.show(\n",
    "    n=5,\n",
    "    truncate=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# take\n",
    "df.take(\n",
    "    num=5,\n",
    ")\n",
    "\n",
    "for row in df.take(5):\n",
    "    row: Row\n",
    "    print(f\"video_id: {row.video_id}\\ntitle: {row.title}\\nlikes: {row.likes}\\nviews: {row.views}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# head\n",
    "df.head(\n",
    "    n=5\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# collect: si los datos son muy grandes, puede provocar un error de memoria (OutOfMemoryError)\n",
    "df.select(col(\"likes\")).collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Escritura de DataFrames en PySpark\n",
    "\n",
    "Una instancia de la clase DataFrameWriter se puede obtener a partir de un DataFrame mediante el método write. Esta clase proporciona métodos para escribir los datos de un DataFrame en un formato determinado.\n",
    "El patrón común para escribir un DataFrame es el siguiente:\n",
    "\n",
    "    `df.write.format(\"formato\").option(\"clave\", \"valor\").partitionBy(\"columna\").bucketBy(n, \"columna\").sortBy(\"columna\").save(\"ruta\")`\n",
    "\n",
    "Donde:\n",
    "* **format**(\"formato\"): especifica el formato en el que se va a guardar el DataFrame. El argumento \"formato\" debe ser una cadena de texto que corresponda a uno de los formatos de archivo soportados por PySpark, como por ejemplo \"parquet\", \"csv\", \"json\", etc.\n",
    "\n",
    "* **option**(\"clave\", \"valor\"): permite establecer una opción específica para el formato de archivo seleccionado en el método anterior. Por ejemplo, si el formato es \"parquet\", una opción común es \"compression\" para especificar el tipo de compresión a utilizar. La \"clave\" es el nombre de la opción y el \"valor\" es el valor que se le asigna.\n",
    "\n",
    "* **partitionBy**(\"columna\"): divide el DataFrame en particiones de acuerdo con los valores únicos de una columna especificada. Esto es útil si se desea guardar los datos en diferentes directorios o archivos según los valores de una columna específica.\n",
    "\n",
    "* **bucketBy**(n, \"columna\"): organiza los datos en \"buckets\" (o cubetas) de acuerdo con los valores de una columna. Esto es útil si se desea particionar los datos de una manera más eficiente para mejorar el rendimiento de ciertas consultas. El argumento \"n\" es el número de cubetas que se desea crear.\n",
    "\n",
    "* **sortBy**(\"columna\"): ordena los datos de acuerdo con los valores de una columna. Esto es útil si se desea que los datos estén ordenados de alguna manera específica antes de guardarlos.\n",
    "\n",
    "* **save**(\"ruta\"): finalmente, guarda el DataFrame en el sistema de archivos en la ruta especificada. La ruta puede ser un directorio o un archivo, dependiendo del formato seleccionado.\n",
    "\n",
    "Existen varios modos de guardado:\n",
    "\n",
    "* **append**: añade los datos al final de un archivo existente.\n",
    "* **overwrite**: sobrescribe el archivo existente.\n",
    "* **ignore**: no hace nada.\n",
    "* **error**: lanza un error.\n",
    "* **errorifexists**: lanza un error si el archivo existe.\n",
    "* **default**: sobrescribe el archivo existente.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = spark.read.parquet(r\"../../data/s7_data/datos.parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T13:21:34.850560400Z",
     "start_time": "2023-05-16T13:21:29.210589300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df.repartition(2)  # Establece el número de particiones del DataFrame\n",
    "\n",
    "# El número de archivos generados es igual al número de particiones del DataFrame\n",
    "\n",
    "# df.write.csv(r\"../../stuff/save_csv.csv\", sep=\"|\")\n",
    "# O\n",
    "# df.write.format(\"csv\").option(\"sep\", \"|\").save(r\"../../stuff/save2_csv.csv\")\n",
    "\n",
    "df.coalesce(1).write.format(\"csv\").option(\"sep\", \"|\").save(r\"../../stuff/save3_csv.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T13:28:33.985521400Z",
     "start_time": "2023-05-16T13:28:31.790900900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Guardar un DF a la vez que se particiona\n",
    "clean_df = df.filter(col(\"comments_disabled\").isin(\"True\", \"False\"))\n",
    "\n",
    "clean_df.write.partitionBy(\"comments_disabled\").parquet(r\"../../stuff/save_parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T13:40:29.591523400Z",
     "start_time": "2023-05-16T13:40:27.093769400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Persistencia de DataFrames en PySpark\n",
    "\n",
    "Al igual que los RDDs, los DataFrames también se pueden persistir en memoria o en disco para mejorar el rendimiento de las consultas que se realicen sobre ellos. Persistir un DF consume menos espacio que persistir un RDD.\n",
    "\n",
    "Para ello, se utiliza el método persist, que acepta como argumento el nivel de persistencia que se desea utilizar. Los niveles de persistencia disponibles son:\n",
    "\n",
    "* **MEMORY_ONLY**: persiste el DataFrame en memoria.\n",
    "* **MEMORY_AND_DISK**: persiste el DataFrame en memoria y en disco.\n",
    "* **MEMORY_ONLY_SER**: persiste el DataFrame en memoria utilizando serialización.\n",
    "* **MEMORY_AND_DISK_SER**: persiste el DataFrame en memoria y en disco utilizando serialización.\n",
    "* **DISK_ONLY**: persiste el DataFrame en disco.\n",
    "* **MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.**: persiste el DataFrame en dos o más nodos.\n",
    "* **OFF_HEAP**: persiste el DataFrame en memoria fuera del heap de Java.\n",
    "* **NONE**: elimina la persistencia del DataFrame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|    a|\n",
      "|  2|    b|\n",
      "|  3|    c|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"a\"), (2, \"b\"), (3, \"c\")], [\"id\", \"value\"])\n",
    "\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T13:47:43.540568700Z",
     "start_time": "2023-05-16T13:47:33.509995800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[id: bigint, value: string]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retirar la persistencia\n",
    "df.unpersist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T13:48:11.181479Z",
     "start_time": "2023-05-16T13:48:11.096359100Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
