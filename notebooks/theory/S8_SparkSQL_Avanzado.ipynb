{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Agregaciones\n",
    "\n",
    "Las agregaciones se refieren a la operación de calcular estadísticas resumidas sobre un conjunto de datos en función de una o varias columnas de agrupación. Las estadísticas que se pueden calcular incluyen la suma, el promedio, el máximo, el mínimo, la desviación estándar, la mediana, el número de valores distintos, entre otras.\n",
    "\n",
    "Las agregaciones se realizan mediante el método groupBy() en un objeto DataFrame, que agrupa los datos en función de una o varias columnas. A continuación, se aplican una o varias funciones de agregación a este objeto GroupedData para calcular las estadísticas resumidas.\n",
    "\n",
    "Sin embargo, no todas las funciones que calculan estadísticas resumidas en PySpark requieren el uso de groupBy(). Por ejemplo, la función count() cuenta el número de filas en un DataFrame sin la necesidad de agrupar por ninguna columna, y se puede aplicar directamente sobre un DataFrame. Por lo tanto, aunque count() es una función de agregación, no requiere el uso de groupBy().\n",
    "\n",
    "En resumen, las agregaciones en PySpark se refieren al cálculo de estadísticas resumidas sobre un conjunto de datos en función de una o varias columnas de agrupación. El uso de la función groupBy() es una forma común de realizar las agregaciones, pero no es necesario en todos los casos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col, desc, sum, sum_distinct, min, max, avg, count, countDistinct, approx_count_distinct, broadcast\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T10:02:03.934695100Z",
     "start_time": "2023-05-17T10:02:03.887280400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mparquet(\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../data/vuelos.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      3\u001B[0m )\n\u001B[0;32m      5\u001B[0m df\u001B[38;5;241m.\u001B[39mprintSchema()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\n",
    "    r\"../../data/vuelos.parquet\",\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T07:13:15.559063200Z",
     "start_time": "2023-05-17T07:13:15.496120100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Funciones count, countDistinct y approx_count_distinct\n",
    "\n",
    "* **count()**: devuelve el número total de filas en un grupo.\n",
    "* **countDistinct()**: devuelve el número de valores distintos de una columna en un grupo.\n",
    "* **approx_count_distinct()**: devuelve el número aproximado de valores distintos de una columna en un grupo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- cantidad: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\n",
    "    r\"../../data/dataframe.parquet\",\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T15:32:01.914740800Z",
     "start_time": "2023-05-16T15:32:01.764452100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+\n",
      "|nombre|color|cantidad|\n",
      "+------+-----+--------+\n",
      "|  Jose| azul|    1900|\n",
      "|  null| null|    1700|\n",
      "|  null| rojo|    1300|\n",
      "|  Juan| rojo|    1500|\n",
      "+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T15:32:12.392570Z",
     "start_time": "2023-05-16T15:32:12.218877800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------+\n",
      "|nombre_count|color_count|general_count|\n",
      "+------------+-----------+-------------+\n",
      "|           2|          3|            4|\n",
      "+------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "df.select(\n",
    "    count(\"nombre\").alias(\"nombre_count\"),\n",
    "    count(\"color\").alias(\"color_count\"),\n",
    "    count(\"*\").alias(\"general_count\"),  # cuenta todas las filas\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T15:35:44.653750400Z",
     "start_time": "2023-05-16T15:35:44.433910900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------+\n",
      "|nombre_count|color_count|general_count|\n",
      "+------------+-----------+-------------+\n",
      "|           2|          2|            2|\n",
      "+------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# countDistinct\n",
    "df.select(\n",
    "    countDistinct(\"nombre\").alias(\"nombre_count\"),\n",
    "    countDistinct(\"color\").alias(\"color_count\"),\n",
    "    countDistinct(\"*\").alias(\"general_count\"),  # cuenta todas las filas\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T15:36:26.780032200Z",
     "start_time": "2023-05-16T15:36:25.915061900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|aerolinea_count|\n",
      "+---------------+\n",
      "|             13|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# approx_count_distinct\n",
    "\n",
    "df_vuelos = spark.read.parquet(\n",
    "    r\"../../data/vuelos.parquet\",\n",
    ")\n",
    "\n",
    "df_vuelos.select(\n",
    "    approx_count_distinct(\"AIRLINE\").alias(\"aerolinea_count\"),\n",
    "    # Es una aproximación del número de valores distintos de la columna AIRLINE en el DataFrame\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T07:14:09.323333800Z",
     "start_time": "2023-05-17T07:13:59.363518200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|aerolinea_count|\n",
      "+---------------+\n",
      "|             14|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# En realidad tiene 14 valores distintos, pero approx_count_distinct devuelve 13\n",
    "df_vuelos.select(\n",
    "    countDistinct(\"AIRLINE\").alias(\"aerolinea_count\"),\n",
    "    # Es una aproximación del número de valores distintos de la columna AIRLINE en el DataFrame\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T15:44:19.105770800Z",
     "start_time": "2023-05-16T15:44:18.134526700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Funciones min y max\n",
    "\n",
    "* **min()**: devuelve el valor mínimo de una columna en un grupo.\n",
    "* **max()**: devuelve el valor máximo de una columna en un grupo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|min_air_time|max_air_time|\n",
      "+------------+------------+\n",
      "|           7|         690|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# min\n",
    "df_vuelos.select(\n",
    "    min(\"AIR_TIME\").alias(\"min_air_time\"),\n",
    "    max(\"AIR_TIME\").alias(\"max_air_time\"),\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T07:14:42.239201200Z",
     "start_time": "2023-05-17T07:14:41.386760500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Funciones sum, sumDistinct y avg\n",
    "\n",
    "* **sum()**: devuelve la suma de los valores de una columna en un grupo.\n",
    "* **sumDistinct()**: devuelve la suma de los valores distintos de una columna en un grupo.\n",
    "* **avg()**: devuelve el promedio de los valores de una columna en un grupo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------+-----------------+\n",
      "|sum_distance|sum_distinct_distance|     avg_distance|\n",
      "+------------+---------------------+-----------------+\n",
      "|  4785357409|              1442300|822.3564947305235|\n",
      "+------------+---------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GroupedData' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 9\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;28msum\u001B[39m, sum_distinct, avg\n\u001B[0;32m      3\u001B[0m df_vuelos\u001B[38;5;241m.\u001B[39mselect(\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28msum\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDISTANCE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msum_distance\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m      5\u001B[0m     sum_distinct(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDISTANCE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msum_distinct_distance\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m      6\u001B[0m     avg(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDISTANCE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mavg_distance\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m      7\u001B[0m )\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m----> 9\u001B[0m \u001B[43mdf_vuelos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mAIRLINE\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m     11\u001B[0m \u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'GroupedData' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "df_vuelos.select(\n",
    "    sum(\"DISTANCE\").alias(\"sum_distance\"),\n",
    "    sum_distinct(\"DISTANCE\").alias(\"sum_distinct_distance\"),\n",
    "    avg(\"DISTANCE\").alias(\"avg_distance\"),\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T07:30:58.549623100Z",
     "start_time": "2023-05-17T07:30:57.539318100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agregación con agrupación\n",
    "\n",
    "Realizar agregación con agrupación es un proceso de dos pasos:\n",
    "\n",
    "1. Realizar la agrupación mediante la transformación groupBy().\n",
    "2. Aplicar una o varias funciones de agregación al objeto GroupedData resultante."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vuelos.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T07:31:46.300567600Z",
     "start_time": "2023-05-17T07:31:46.202329600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|ORIGIN_AIRPORT| count|\n",
      "+--------------+------+\n",
      "|           ATL|346836|\n",
      "|           ORD|285884|\n",
      "|           DFW|239551|\n",
      "|           DEN|196055|\n",
      "|           LAX|194673|\n",
      "|           SFO|148008|\n",
      "|           PHX|146815|\n",
      "|           IAH|146622|\n",
      "|           LAS|133181|\n",
      "|           MSP|112117|\n",
      "|           MCO|110982|\n",
      "|           SEA|110899|\n",
      "|           DTW|108500|\n",
      "|           BOS|107847|\n",
      "|           EWR|101772|\n",
      "|           CLT|100324|\n",
      "|           LGA| 99605|\n",
      "|           SLC| 97210|\n",
      "|           JFK| 93811|\n",
      "|           BWI| 86079|\n",
      "+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupación por aeropuerto de origen\n",
    "df_vuelos.groupBy(\"ORIGIN_AIRPORT\").count().orderBy(desc(\"count\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T07:34:36.210827Z",
     "start_time": "2023-05-17T07:34:34.690873600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-----+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|count|\n",
      "+--------------+-------------------+-----+\n",
      "|           SFO|                LAX|13744|\n",
      "|           LAX|                SFO|13457|\n",
      "|           JFK|                LAX|12016|\n",
      "|           LAX|                JFK|12015|\n",
      "|           LAS|                LAX| 9715|\n",
      "|           LGA|                ORD| 9639|\n",
      "|           LAX|                LAS| 9594|\n",
      "|           ORD|                LGA| 9575|\n",
      "|           SFO|                JFK| 8440|\n",
      "|           JFK|                SFO| 8437|\n",
      "|           OGG|                HNL| 8313|\n",
      "|           HNL|                OGG| 8282|\n",
      "|           LAX|                ORD| 8256|\n",
      "|           ATL|                LGA| 8234|\n",
      "|           LGA|                ATL| 8215|\n",
      "|           ATL|                MCO| 8202|\n",
      "|           MCO|                ATL| 8202|\n",
      "|           SFO|                LAS| 7995|\n",
      "|           ORD|                LAX| 7941|\n",
      "|           LAS|                SFO| 7870|\n",
      "+--------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_vuelos.\n",
    "    groupBy(\n",
    "        \"ORIGIN_AIRPORT\",\n",
    "        \"DESTINATION_AIRPORT\"\n",
    "    )\n",
    "    .count()\n",
    "    .orderBy(\n",
    "        desc(\"count\")\n",
    "    )\n",
    "    .show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T07:39:49.051219500Z",
     "start_time": "2023-05-17T07:39:47.280766Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vuelos.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T07:47:39.990052900Z",
     "start_time": "2023-05-17T07:47:39.953354200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+------------------+------------+------------+\n",
      "|ORIGIN_AIRPORT| count|      avg_distance|min_distance|max_distance|\n",
      "+--------------+------+------------------+------------+------------+\n",
      "|         12016|    31|            3801.0|        3801|        3801|\n",
      "|           GUM|   334|            3801.0|        3801|        3801|\n",
      "|           PPG|   107|            2599.0|        2599|        2599|\n",
      "|         14222|     9|            2599.0|        2599|        2599|\n",
      "|         12478|  8303|1476.6809586896302|         173|        4983|\n",
      "|           JFK| 93811|1441.8611463474433|         173|        4983|\n",
      "|         14254|    62|            1398.0|        1179|        1617|\n",
      "|           ANC| 16005|1397.8466104342392|         160|        3417|\n",
      "|           SJU| 24656|1385.0269305645684|          68|        2404|\n",
      "|           PSE|   749|1373.7316421895862|        1179|        1617|\n",
      "|         14843|  1868| 1368.460920770878|          68|        2165|\n",
      "|         12264|  2996|1365.3200934579438|         212|        4817|\n",
      "|           BQN|  1343|1358.6612062546537|         982|        1585|\n",
      "|         10732|    96|         1353.1875|         982|        1585|\n",
      "|         14025|    13|            1334.0|        1334|        1334|\n",
      "|         10299|  1234|1324.9108589951377|         160|        3043|\n",
      "|           IAD| 34305|1293.5235971432735|         100|        4817|\n",
      "|           HNL| 43172|1271.8382748077458|         100|        4983|\n",
      "|         12892| 17728|1269.6750338447653|          36|        2615|\n",
      "|           SEA|110899|1267.6722513277848|          93|        2724|\n",
      "+--------------+------+------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Múltiples agregaciones por grupo\n",
    "(\n",
    "    df_vuelos.\n",
    "    groupBy(\n",
    "        \"ORIGIN_AIRPORT\",\n",
    "    )\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        avg(\"DISTANCE\").alias(\"avg_distance\"),\n",
    "        min(\"DISTANCE\").alias(\"min_distance\"),\n",
    "        max(\"DISTANCE\").alias(\"max_distance\"),\n",
    "    )\n",
    "    .orderBy(\n",
    "        desc(\"avg_distance\"),\n",
    "    )\n",
    "    .show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T07:48:27.987425100Z",
     "start_time": "2023-05-17T07:48:26.595203700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------------+\n",
      "|MONTH| count|  avg_arrival_delay|\n",
      "+-----+------+-------------------+\n",
      "|    1|469968|  5.813582983416227|\n",
      "|    2|429191|  8.320500020850556|\n",
      "|    3|504312|   4.92067265685641|\n",
      "|    4|485151| 3.1631900611579318|\n",
      "|    5|496993|  4.485018615679651|\n",
      "|    6|503897|  9.601590351569554|\n",
      "|    7|520718| 6.4317747052785466|\n",
      "|    8|510536|  4.607372469025074|\n",
      "|    9|464946|-0.7725709883956179|\n",
      "|   10|486165| -0.780389663641748|\n",
      "|   11|467972|  1.100783576682592|\n",
      "|   12|479230|  6.092902747824754|\n",
      "+-----+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_vuelos\n",
    "    .groupby(\n",
    "        \"MONTH\",\n",
    "    )\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        avg(\"ARRIVAL_DELAY\").alias(\"avg_arrival_delay\"),\n",
    "    )\n",
    "    .orderBy(\n",
    "        \"MONTH\",\n",
    "    )\n",
    "    .show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T07:50:37.249883300Z",
     "start_time": "2023-05-17T07:50:36.698435900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agregación con Pivote\n",
    "\n",
    "La función pivot en PySpark se utiliza para realizar una operación de pivote en un DataFrame. El pivote es una operación que permite reorganizar los datos en un DataFrame, convirtiendo los valores de una columna en nuevas columnas.\n",
    "\n",
    "En la operación de pivote, debes especificar tres elementos principales:\n",
    "\n",
    "1. La columna que se utilizará como referencia para crear las nuevas columnas.\n",
    "2. Los valores únicos de esa columna que se convertirán en las nuevas columnas.\n",
    "3. La columna cuyos valores se agregarán en las nuevas columnas creadas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- sexo: string (nullable = true)\n",
      " |-- peso: long (nullable = true)\n",
      " |-- graduacion: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_estudiantes = spark.read.parquet(\n",
    "    r\"../../data/estudiantes.parquet\",\n",
    ")\n",
    "df_estudiantes.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:01:40.881029Z",
     "start_time": "2023-05-17T08:01:40.735702800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+----------+\n",
      "|nombre|sexo|peso|graduacion|\n",
      "+------+----+----+----------+\n",
      "|  Jose|   M|  80|      2000|\n",
      "| Hilda|   F|  50|      2000|\n",
      "|  Juan|   M|  75|      2000|\n",
      "| Pedro|   M|  76|      2001|\n",
      "|Katia+|   F|  65|      2001|\n",
      "+------+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_estudiantes.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:01:49.443729700Z",
     "start_time": "2023-05-17T08:01:49.252064400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+\n",
      "|graduacion|M_avg_peso|M_min_peso|M_max_peso|\n",
      "+----------+----------+----------+----------+\n",
      "|      2000|      77.5|        75|        80|\n",
      "|      2001|      76.0|        76|        76|\n",
      "+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# avg, min, max de peso por año de graduación y sexo\n",
    "\n",
    "(\n",
    "    df_estudiantes\n",
    "    .groupBy(\n",
    "        \"graduacion\",\n",
    "    )\n",
    "    .pivot(\n",
    "        \"sexo\",\n",
    "    )\n",
    "    .agg(\n",
    "        avg(\"peso\").alias(\"avg_peso\"),\n",
    "        min(\"peso\").alias(\"min_peso\"),\n",
    "        max(\"peso\").alias(\"max_peso\"),\n",
    "    )\n",
    "    .orderBy(\n",
    "        \"graduacion\",\n",
    "    )\n",
    "    .show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:16:06.979897400Z",
     "start_time": "2023-05-17T08:16:06.634603Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+\n",
      "|graduacion|M_avg_peso|M_min_peso|M_max_peso|\n",
      "+----------+----------+----------+----------+\n",
      "|      2000|      77.5|        75|        80|\n",
      "|      2001|      76.0|        76|        76|\n",
      "+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivot con ciertos valores\n",
    "\n",
    "(\n",
    "    df_estudiantes\n",
    "    .groupBy(\n",
    "        \"graduacion\",\n",
    "    )\n",
    "    .pivot(\n",
    "        pivot_col=\"sexo\",\n",
    "        values=[\"M\"],  # solo se toman en cuenta los valores M de la columna sexo\n",
    "    )\n",
    "    .agg(\n",
    "        avg(\"peso\").alias(\"avg_peso\"),\n",
    "        min(\"peso\").alias(\"min_peso\"),\n",
    "        max(\"peso\").alias(\"max_peso\"),\n",
    "    )\n",
    "    .orderBy(\n",
    "        \"graduacion\",\n",
    "    )\n",
    "    .show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:17:21.240255900Z",
     "start_time": "2023-05-17T08:17:20.953586900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Joins\n",
    "\n",
    "Un join es una operación que combina dos DataFrames basándose en una columna común. La columna común se utiliza para comparar los valores en ambas tablas y unir las filas que coincidan según un criterio específico.\n",
    "\n",
    "Existen varios tipos de join en PySpark, que determinan cómo se realizan las combinaciones de filas entre los DataFrames. A continuación, te explico los tipos de join más comunes:\n",
    "\n",
    "* **Inner Join**: Es el tipo de join más común. Solo incluye las filas en el resultado final que tienen una coincidencia en ambas tablas en la columna especificada. En otras palabras, se seleccionan solo las filas que tienen un valor común en la columna utilizada para el join.\n",
    "\n",
    "* **Left Join**: Incluye todas las filas del DataFrame izquierdo (el primer DataFrame en la operación join) y las filas coincidentes del DataFrame derecho. Si no hay coincidencias en el DataFrame derecho, se rellenan con valores nulos.\n",
    "\n",
    "* **Right Join**: Es similar al left join, pero incluye todas las filas del DataFrame derecho y las filas coincidentes del DataFrame izquierdo. Si no hay coincidencias en el DataFrame izquierdo, se rellenan con valores nulos.\n",
    "\n",
    "* **Full Outer Join**: Incluye todas las filas de ambos DataFrames en el resultado final, y rellena con valores nulos en las columnas correspondientes si no hay coincidencias.\n",
    "\n",
    "* **Left Semi Join**: Devuelve las filas del DataFrame izquierdo que tienen coincidencias en el DataFrame derecho, sin incluir realmente las columnas del DataFrame derecho en el resultado.\n",
    "\n",
    "* **Left Anti Join**: Devuelve las filas del DataFrame izquierdo que no tienen coincidencias en el DataFrame derecho.\n",
    "\n",
    "* **Cross Join**: Devuelve el producto cartesiano de ambos DataFrames. Es decir, combina cada fila del DataFrame izquierdo con cada fila del DataFrame derecho."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "df_empleados = spark.read.parquet(\n",
    "    r\"../../data/empleados.parquet\",\n",
    ")\n",
    "\n",
    "df_departamentos = spark.read.parquet(\n",
    "    r\"../../data/departamentos.parquet\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:30:17.250769100Z",
     "start_time": "2023-05-17T08:30:17.038469200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- num_dpto: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empleados.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:30:20.959692100Z",
     "start_time": "2023-05-17T08:30:20.904174500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre_dpto: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_departamentos.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:30:30.479932800Z",
     "start_time": "2023-05-17T08:30:30.400202700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|nombre|num_dpto|\n",
      "+------+--------+\n",
      "|  Luis|      33|\n",
      "| Katia|      33|\n",
      "|  Raul|      34|\n",
      "| Pedro|       0|\n",
      "| Laura|      34|\n",
      "|Sandro|      31|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empleados.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:30:36.970541400Z",
     "start_time": "2023-05-17T08:30:36.860705200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|nombre_dpto|\n",
      "+---+-----------+\n",
      "| 31|     letras|\n",
      "| 33|    derecho|\n",
      "| 34| matemática|\n",
      "| 35|informática|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_departamentos.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:30:39.727613100Z",
     "start_time": "2023-05-17T08:30:39.514403900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Raul|      34| 34| matemática|\n",
      "| Laura|      34| 34| matemática|\n",
      "|Sandro|      31| 31|     letras|\n",
      "+------+--------+---+-----------+\n",
      "\n",
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Raul|      34| 34| matemática|\n",
      "| Laura|      34| 34| matemática|\n",
      "|Sandro|      31| 31|     letras|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inner Join\n",
    "(\n",
    "    df_empleados\n",
    "    .join(\n",
    "        df_departamentos,\n",
    "        on=df_empleados.num_dpto == df_departamentos.id,\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .show()\n",
    ")\n",
    "# O\n",
    "(\n",
    "    df_empleados\n",
    "    .join(\n",
    "        df_departamentos,\n",
    "    )\n",
    "    .where(\n",
    "        df_empleados.num_dpto == df_departamentos.id,\n",
    "    )\n",
    "    .show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:41:15.437583300Z",
     "start_time": "2023-05-17T08:41:14.963278800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+-----------+\n",
      "|nombre|num_dpto|  id|nombre_dpto|\n",
      "+------+--------+----+-----------+\n",
      "|  Luis|      33|  33|    derecho|\n",
      "| Katia|      33|  33|    derecho|\n",
      "|  Raul|      34|  34| matemática|\n",
      "| Pedro|       0|null|       null|\n",
      "| Laura|      34|  34| matemática|\n",
      "|Sandro|      31|  31|     letras|\n",
      "+------+--------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Left Outer Join\n",
    "\n",
    "(\n",
    "    df_empleados\n",
    "    .join(\n",
    "        df_departamentos,\n",
    "        on=df_empleados.num_dpto == df_departamentos.id,\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:41:57.200981600Z",
     "start_time": "2023-05-17T08:41:57.014171400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|Sandro|      31| 31|     letras|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Laura|      34| 34| matemática|\n",
      "|  Raul|      34| 34| matemática|\n",
      "|  null|    null| 35|informática|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Right Outer Join\n",
    "\n",
    "(\n",
    "    df_empleados\n",
    "    .join(\n",
    "        df_departamentos,\n",
    "        on=df_empleados.num_dpto == df_departamentos.id,\n",
    "        how=\"right\",\n",
    "    )\n",
    "    .show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:42:17.406009600Z",
     "start_time": "2023-05-17T08:42:17.188089500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+-----------+\n",
      "|nombre|num_dpto|  id|nombre_dpto|\n",
      "+------+--------+----+-----------+\n",
      "| Pedro|       0|null|       null|\n",
      "|Sandro|      31|  31|     letras|\n",
      "|  Luis|      33|  33|    derecho|\n",
      "| Katia|      33|  33|    derecho|\n",
      "|  Raul|      34|  34| matemática|\n",
      "| Laura|      34|  34| matemática|\n",
      "|  null|    null|  35|informática|\n",
      "+------+--------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Full Outer Join\n",
    "\n",
    "(\n",
    "    df_empleados\n",
    "    .join(\n",
    "        df_departamentos,\n",
    "        on=df_empleados.num_dpto == df_departamentos.id,\n",
    "        how=\"full\",\n",
    "    )\n",
    "    .show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:42:24.060092800Z",
     "start_time": "2023-05-17T08:42:23.683278100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|nombre|num_dpto|\n",
      "+------+--------+\n",
      "|  Luis|      33|\n",
      "| Katia|      33|\n",
      "|  Raul|      34|\n",
      "| Laura|      34|\n",
      "|Sandro|      31|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Left Semi Join\n",
    "\n",
    "(\n",
    "    df_empleados\n",
    "    .join(\n",
    "        df_departamentos,\n",
    "        on=df_empleados.num_dpto == df_departamentos.id,\n",
    "        how=\"left_semi\",\n",
    "    )\n",
    "    .show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:42:56.757136600Z",
     "start_time": "2023-05-17T08:42:56.537785500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|nombre|num_dpto|\n",
      "+------+--------+\n",
      "| Pedro|       0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Left Anti Join\n",
    "\n",
    "(\n",
    "    df_empleados\n",
    "    .join(\n",
    "        df_departamentos,\n",
    "        on=df_empleados.num_dpto == df_departamentos.id,\n",
    "        how=\"left_anti\",\n",
    "    )\n",
    "    .show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T08:43:26.498292700Z",
     "start_time": "2023-05-17T08:43:26.308746800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|  Luis|      33| 31|     letras|\n",
      "|  Luis|      33| 33|    derecho|\n",
      "|  Luis|      33| 34| matemática|\n",
      "|  Luis|      33| 35|informática|\n",
      "| Katia|      33| 31|     letras|\n",
      "| Katia|      33| 33|    derecho|\n",
      "| Katia|      33| 34| matemática|\n",
      "| Katia|      33| 35|informática|\n",
      "|  Raul|      34| 31|     letras|\n",
      "|  Raul|      34| 33|    derecho|\n",
      "|  Raul|      34| 34| matemática|\n",
      "|  Raul|      34| 35|informática|\n",
      "| Pedro|       0| 31|     letras|\n",
      "| Pedro|       0| 33|    derecho|\n",
      "| Pedro|       0| 34| matemática|\n",
      "| Pedro|       0| 35|informática|\n",
      "| Laura|      34| 31|     letras|\n",
      "| Laura|      34| 33|    derecho|\n",
      "| Laura|      34| 34| matemática|\n",
      "| Laura|      34| 35|informática|\n",
      "+------+--------+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross Join\n",
    "\n",
    "(\n",
    "    df_empleados\n",
    "    .crossJoin(\n",
    "        df_departamentos,\n",
    "    )\n",
    "    .show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T09:07:44.953829900Z",
     "start_time": "2023-05-17T09:07:44.740549300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Manejo de nombres de columnas duplicados"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre_dpto: string (nullable = true)\n",
      " |-- num_dpto: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- num_dpto: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_depa = df_departamentos.withColumn(\"num_dpto\", col(\"id\"))\n",
    "\n",
    "df_depa.printSchema()\n",
    "\n",
    "df_empleados.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T09:33:32.104397900Z",
     "start_time": "2023-05-17T09:33:32.072834600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Reference 'num_dpto' is ambiguous, could be: num_dpto, num_dpto.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[62], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Join error\u001B[39;00m\n\u001B[0;32m      3\u001B[0m (\n\u001B[1;32m----> 4\u001B[0m     \u001B[43mdf_empleados\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdf_depa\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43mon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnum_dpto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnum_dpto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minner\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;241m.\u001B[39mshow()\n\u001B[0;32m     11\u001B[0m )\n",
      "File \u001B[1;32mb:\\python-venvs\\pyspark-venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1355\u001B[0m, in \u001B[0;36mDataFrame.join\u001B[1;34m(self, other, on, how)\u001B[0m\n\u001B[0;32m   1353\u001B[0m         on \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jseq([])\n\u001B[0;32m   1354\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(how, \u001B[38;5;28mstr\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow should be a string\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1355\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1356\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msql_ctx)\n",
      "File \u001B[1;32mb:\\python-venvs\\pyspark-venv\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[1;32mb:\\python-venvs\\pyspark-venv\\lib\\site-packages\\pyspark\\sql\\utils.py:117\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    113\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: Reference 'num_dpto' is ambiguous, could be: num_dpto, num_dpto."
     ]
    }
   ],
   "source": [
    "# Join error\n",
    "\n",
    "(\n",
    "    df_empleados\n",
    "    .join(\n",
    "        df_depa,\n",
    "        on=col(\"num_dpto\") == col(\"num_dpto\"),\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T09:44:02.351694100Z",
     "start_time": "2023-05-17T09:44:02.191602300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+--------+\n",
      "|nombre|num_dpto| id|nombre_dpto|num_dpto|\n",
      "+------+--------+---+-----------+--------+\n",
      "|Sandro|      31| 31|     letras|      31|\n",
      "| Katia|      33| 33|    derecho|      33|\n",
      "|  Luis|      33| 33|    derecho|      33|\n",
      "| Laura|      34| 34| matemática|      34|\n",
      "|  Raul|      34| 34| matemática|      34|\n",
      "+------+--------+---+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Especificando el DF\n",
    "\n",
    "df_result = (\n",
    "    df_empleados\n",
    "    .join(\n",
    "        df_depa,\n",
    "        on=df_empleados.num_dpto == df_depa.num_dpto,  # o df_empleados[\"num_dpto\"] == df_depa[\"num_dpto\"]\n",
    "        how=\"inner\",\n",
    "    )\n",
    ")\n",
    "\n",
    "df_result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T09:46:43.412035200Z",
     "start_time": "2023-05-17T09:46:43.208839500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Reference 'num_dpto' is ambiguous, could be: num_dpto, num_dpto.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[69], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Select error\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mdf_result\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnum_dpto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[1;32mb:\\python-venvs\\pyspark-venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1685\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[1;34m(self, *cols)\u001B[0m\n\u001B[0;32m   1664\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols):\n\u001B[0;32m   1665\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[0;32m   1666\u001B[0m \n\u001B[0;32m   1667\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1683\u001B[0m \u001B[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001B[39;00m\n\u001B[0;32m   1684\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1685\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1686\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msql_ctx)\n",
      "File \u001B[1;32mb:\\python-venvs\\pyspark-venv\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[1;32mb:\\python-venvs\\pyspark-venv\\lib\\site-packages\\pyspark\\sql\\utils.py:117\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    113\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: Reference 'num_dpto' is ambiguous, could be: num_dpto, num_dpto."
     ]
    }
   ],
   "source": [
    "# Select error\n",
    "\n",
    "df_result.select(\"num_dpto\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T09:46:45.206056800Z",
     "start_time": "2023-05-17T09:46:45.110635500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|num_dpto|\n",
      "+--------+\n",
      "|      33|\n",
      "|      33|\n",
      "|      34|\n",
      "|      34|\n",
      "|      31|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select con DF especificado\n",
    "\n",
    "df_result.select(df_empleados.num_dpto).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T09:48:27.212704100Z",
     "start_time": "2023-05-17T09:48:26.975931800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# Utilizando una columna de Union\n",
    "\n",
    "dummy_df = df_empleados.join(\n",
    "    df_depa,\n",
    "    on=\"num_dpto\",\n",
    "    how=\"inner\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T09:51:31.805894400Z",
     "start_time": "2023-05-17T09:51:31.727269300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num_dpto: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre_dpto: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df.printSchema()  # num_dpto es la columna de union, solo aparece una vez"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T09:51:32.996848500Z",
     "start_time": "2023-05-17T09:51:32.952038400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Shuffle Hash Join y Broadcast Hash Join\n",
    "\n",
    "El shuffle y el broadcast hash join son técnicas utilizadas para optimizar las operaciones de unión (join) entre DataFrames o RDDs distribuidos. A continuación, se explica cada uno:\n",
    "\n",
    "* **Shuffle**: Shuffle es una operación costosa que implica la redistribución de datos entre los nodos del clúster. En el contexto de un join, implica la reorganización de los datos para asegurar que las claves de unión se encuentren en el mismo nodo. Esto es necesario para que los registros con las mismas claves se combinen correctamente.\n",
    "\n",
    "Durante un shuffle, los datos se particionan y se envían a diferentes nodos según un criterio de particionamiento, como una función de hash de las claves de unión. Luego, los nodos realizan la combinación de los registros que tienen las mismas claves. El shuffle puede ser intensivo en términos de tiempo y recursos, ya que requiere transferir y reorganizar grandes volúmenes de datos entre los nodos.\n",
    "\n",
    "* **Broadcast Hash Join**: En contraste con el shuffle, el broadcast hash join es una estrategia de optimización que evita la necesidad de realizar un shuffle. En este caso, una de las tablas involucradas en el join (la tabla más pequeña) se \"broadcastea\" o se distribuye a todos los nodos del clúster. Esto significa que cada nodo tiene una copia local de la tabla pequeña.\n",
    "\n",
    "Al tener la tabla pequeña en cada nodo, el join se realiza localmente en cada nodo sin necesidad de reorganizar datos o realizar transferencias entre nodos. Este enfoque es eficiente cuando una de las tablas es lo suficientemente pequeña para caber en la memoria de cada nodo.\n",
    "\n",
    "Sin embargo, es importante tener en cuenta que el broadcast hash join solo es beneficioso cuando la tabla pequeña es lo suficientemente pequeña para caber en la memoria de cada nodo. Si la tabla pequeña es demasiado grande, puede llevar a problemas de rendimiento debido al consumo excesivo de memoria en cada nodo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- num_dpto: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre_dpto: string (nullable = true)\n",
      "\n",
      "+------+--------+\n",
      "|nombre|num_dpto|\n",
      "+------+--------+\n",
      "|  Luis|      33|\n",
      "| Katia|      33|\n",
      "|  Raul|      34|\n",
      "| Pedro|       0|\n",
      "| Laura|      34|\n",
      "|Sandro|      31|\n",
      "+------+--------+\n",
      "\n",
      "+---+-----------+\n",
      "| id|nombre_dpto|\n",
      "+---+-----------+\n",
      "| 31|     letras|\n",
      "| 33|    derecho|\n",
      "| 34| matemática|\n",
      "| 35|informática|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empleados = spark.read.parquet(\n",
    "    r\"../../data/empleados.parquet\",\n",
    ")\n",
    "\n",
    "df_departamentos = spark.read.parquet(\n",
    "    r\"../../data/departamentos.parquet\",\n",
    ")\n",
    "\n",
    "df_empleados.printSchema()\n",
    "\n",
    "df_departamentos.printSchema()\n",
    "\n",
    "df_empleados.show()\n",
    "\n",
    "df_departamentos.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T10:01:05.345588800Z",
     "start_time": "2023-05-17T10:01:04.883958Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [num_dpto#6116L], [id#6119L], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(num_dpto#6116L)\n",
      "   :  +- FileScan parquet [nombre#6115,num_dpto#6116L] Batched: true, DataFilters: [isnotnull(num_dpto#6116L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/B:/OneDrive/Coding/Python/Courses/PySpark/ingenieria-datos-pytho..., PartitionFilters: [], PushedFilters: [IsNotNull(num_dpto)], ReadSchema: struct<nombre:string,num_dpto:bigint>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=3196]\n",
      "      +- Filter isnotnull(id#6119L)\n",
      "         +- FileScan parquet [id#6119L,nombre_dpto#6120] Batched: true, DataFilters: [isnotnull(id#6119L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/B:/OneDrive/Coding/Python/Courses/PySpark/ingenieria-datos-pytho..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint,nombre_dpto:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shuffle Hash Join\n",
    "\n",
    "(\n",
    "    df_empleados\n",
    "    .join(\n",
    "        df_departamentos,\n",
    "        on=df_empleados.num_dpto == df_departamentos.id,\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .explain()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T10:01:23.428827500Z",
     "start_time": "2023-05-17T10:01:23.365891300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [num_dpto#6116L], [id#6119L], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(num_dpto#6116L)\n",
      "   :  +- FileScan parquet [nombre#6115,num_dpto#6116L] Batched: true, DataFilters: [isnotnull(num_dpto#6116L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/B:/OneDrive/Coding/Python/Courses/PySpark/ingenieria-datos-pytho..., PartitionFilters: [], PushedFilters: [IsNotNull(num_dpto)], ReadSchema: struct<nombre:string,num_dpto:bigint>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=3219]\n",
      "      +- Filter isnotnull(id#6119L)\n",
      "         +- FileScan parquet [id#6119L,nombre_dpto#6120] Batched: true, DataFilters: [isnotnull(id#6119L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/B:/OneDrive/Coding/Python/Courses/PySpark/ingenieria-datos-pytho..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint,nombre_dpto:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Broadcast Hash Join\n",
    "\n",
    "(\n",
    "    df_empleados\n",
    "    .join(\n",
    "        broadcast(df_departamentos),\n",
    "        on=df_empleados.num_dpto == df_departamentos.id,\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .explain()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T10:02:10.760442500Z",
     "start_time": "2023-05-17T10:02:10.666118800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
