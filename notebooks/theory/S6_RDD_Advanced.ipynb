{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Almacenamiento en caché\n",
    "El almacenamiento en caché (caching) de RDD en Spark es una técnica que se utiliza para mejorar el rendimiento de las aplicaciones de Spark que realizan múltiples operaciones sobre los mismos RDDs.\n",
    "\n",
    "Cuando se realiza una operación sobre un RDD en Spark, como una transformación o una acción, Spark calcula el resultado y lo almacena en la memoria del cluster de Spark. Si posteriormente se realiza otra operación sobre el mismo RDD, Spark tiene que volver a calcular el resultado desde cero. Esto puede ser muy costoso en términos de tiempo de cómputo y puede reducir el rendimiento de la aplicación.\n",
    "\n",
    "El almacenamiento en caché de RDD resuelve este problema al permitir que los resultados intermedios de un RDD se almacenen en memoria y se reutilicen en operaciones posteriores. Esto se logra mediante la llamada a la función cache() o persist() en un RDD. La función cache() almacena el RDD en memoria, mientras que la función persist() permite al usuario especificar un nivel de almacenamiento en disco o en memoria.\n",
    "\n",
    "Cuando se almacena en caché un RDD, los resultados intermedios se almacenan en la memoria del cluster de Spark y se reutilizan en operaciones posteriores en lugar de tener que recalcularlos desde cero. Esto puede mejorar significativamente el rendimiento de las aplicaciones de Spark que realizan múltiples operaciones sobre los mismos RDDs.\n",
    "\n",
    "Es importante tener en cuenta que la caché de RDDs puede ocupar una gran cantidad de memoria del cluster de Spark, por lo que se recomienda utilizarla con precaución y liberar la memoria caché cuando ya no se necesite, utilizando la función unpersist(). Además, la caché de RDDs puede ser especialmente útil cuando se realizan múltiples operaciones sobre un RDD grande, ya que permite reutilizar los resultados intermedios en lugar de volver a calcularlos cada vez.\n",
    "\n",
    "## Niveles de almacenamiento\n",
    "**MEMORY_ONLY**: Almacena el RDD en la memoria del cluster de Spark. Si el RDD no cabe en la memoria, algunas particiones no se almacenarán y tendrán que ser recalculadas cuando sea necesario. Es el nivel de almacenamiento por defecto.\n",
    "\n",
    "**MEMORY_AND_DISK**: Almacena el RDD en la memoria del cluster de Spark y en el disco. Si el RDD no cabe en la memoria, algunas particiones se almacenarán en el disco en lugar de la memoria.\n",
    "\n",
    "**DISK_ONLY**: Almacena el RDD en el disco del cluster de Spark.\n",
    "\n",
    "**MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.**: Almacena el RDD en la memoria y/o en el disco del cluster de Spark, replicando cada partición en dos nodos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T09:46:36.450419300Z",
     "start_time": "2023-05-12T09:46:36.440507600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(100000000))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T09:46:36.500458500Z",
     "start_time": "2023-05-12T09:46:36.450419300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "PythonRDD[5] at RDD at PythonRDD.scala:53"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.persist(StorageLevel.MEMORY_ONLY)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T09:46:36.520584100Z",
     "start_time": "2023-05-12T09:46:36.480143900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "PythonRDD[5] at RDD at PythonRDD.scala:53"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Es necesario unpersistir el RDD cuando se quiera cambiar el nivel de almacenamiento, o arrojará un error\n",
    "rdd.unpersist()\n",
    "rdd.persist(StorageLevel.MEMORY_AND_DISK)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T09:46:36.575774500Z",
     "start_time": "2023-05-12T09:46:36.515565400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "PythonRDD[5] at RDD at PythonRDD.scala:53"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.unpersist()\n",
    "rdd.cache()  # Equivalente a rdd.persist(StorageLevel.MEMORY_ONLY)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T09:48:07.228250400Z",
     "start_time": "2023-05-12T09:48:07.190883Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Particionado de RDD\n",
    "El particionado de RDD en PySpark se refiere a la forma en que se divide un RDD en partes más pequeñas llamadas particiones. Las particiones son la unidad básica de procesamiento en PySpark y pueden ser procesadas de forma independiente en diferentes nodos de un cluster de Spark.\n",
    "\n",
    "La forma en que un RDD se particiona tiene un gran impacto en el rendimiento de las aplicaciones de PySpark. Si el RDD está mal particionado, es decir, si las particiones son demasiado grandes o demasiado pequeñas, puede haber un desequilibrio en la carga de trabajo entre los nodos del cluster de Spark, lo que puede afectar el rendimiento de la aplicación.\n",
    "\n",
    "En PySpark, existen dos tipos de particionado de RDD: el particionado por defecto y el particionado personalizado.\n",
    "\n",
    "El particionado por defecto se realiza automáticamente cuando se crea un RDD a partir de datos, como un archivo de texto o una tabla de base de datos. En este caso, PySpark utiliza un particionado predeterminado, que se basa en el número de núcleos de CPU en el cluster de Spark.\n",
    "\n",
    "El particionado personalizado, por otro lado, permite al usuario controlar la forma en que un RDD se particiona. Para particionar un RDD de forma personalizada, se puede utilizar la función repartition() o coalesce() de PySpark.\n",
    "\n",
    "Es importante tener en cuenta que la partición de RDD debe ser cuidadosamente seleccionada para optimizar el rendimiento de la aplicación. Un número excesivo de particiones puede aumentar el tiempo de latencia y reducir el rendimiento, mientras que un número insuficiente de particiones puede disminuir la capacidad de procesamiento paralelo y aumentar el tiempo de ejecución.\n",
    "\n",
    "## Particionadores\n",
    "Un particionador es una función que se utiliza para asignar claves a particiones. Los particionadores son útiles en operaciones de agrupación y ordenamiento, donde los datos se deben agrupar o ordenar según una clave específica.\n",
    "\n",
    "Hay dos tipos de particionadores en Spark: el **particionador hash** y el **particionador de rango (o range)**.\n",
    "\n",
    "El particionador hash es el particionador predeterminado en Spark. Este particionador utiliza una función de hash para asignar las claves a particiones. La función hash toma una clave y la convierte en un número entero. Luego, el particionador divide el rango de números enteros en un número determinado de particiones, y asigna las claves a las particiones basándose en el número de la función hash.\n",
    "\n",
    "El particionador de rango, por otro lado, asigna las claves a las particiones basándose en el rango de valores de las claves. Por ejemplo, si se tiene un RDD de tuplas (k, v) y se desea ordenar el RDD por la clave k, el particionador de rango asignará las claves a las particiones basándose en el rango de valores de k. Cada partición tendrá un rango de valores de k específico, y las claves se asignarán a la partición correspondiente en función del rango.\n",
    "\n",
    "El particionador de rango es especialmente útil cuando se trabaja con datos ordenados, ya que se puede garantizar que los datos con claves cercanas estarán en la misma partición. Esto puede mejorar el rendimiento de operaciones como la unión de RDD, la intersección y la operación de join.\n",
    "\n",
    "En general, el particionador a utilizar dependerá del tipo de datos y la operación que se va a realizar. En muchos casos, el particionador predeterminado (hash) funciona bien, pero en otros casos, es necesario utilizar un particionador de rango personalizado para lograr el mejor rendimiento posible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Hash partitioner\n",
    "rdd = sc.parallelize([\"x\", \"y\", \"z\"])\n",
    "partitions = rdd.getNumPartitions()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T10:02:43.263071200Z",
     "start_time": "2023-05-12T10:02:43.198747400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 11 5\n"
     ]
    }
   ],
   "source": [
    "# index = hash(key) % partitions\n",
    "index_x = hash(\"x\") % partitions\n",
    "index_y = hash(\"y\") % partitions\n",
    "index_z = hash(\"z\") % partitions\n",
    "\n",
    "print(index_x, index_y, index_z)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T10:02:46.260425300Z",
     "start_time": "2023-05-12T10:02:46.246729900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Shuffling en PySpark\n",
    "El shuffling es una operación de procesamiento en Spark que implica la reorganización de los datos en un RDD (Resilient Distributed Dataset) a través de la red. El shuffling es una operación costosa, ya que requiere que los datos se muevan de un nodo de la red a otro, lo que puede llevar tiempo y consumir una gran cantidad de recursos de red y procesamiento.\n",
    "\n",
    "En términos generales, el shuffling se utiliza cuando se necesita reorganizar los datos en un RDD de tal manera que los datos con la misma clave se encuentren en la misma partición. Por ejemplo, al realizar una operación de groupBy en un RDD, los datos deben agruparse por clave en cada partición y luego combinarse en una sola partición. Esto implica una operación de shuffling, ya que los datos deben ser reorganizados de tal manera que los datos con la misma clave se encuentren en la misma partición.\n",
    "\n",
    "El proceso de shuffling en Spark implica tres fases principales:\n",
    "\n",
    "Particionamiento: los datos del RDD se asignan a particiones en función de una clave.\n",
    "\n",
    "Transferencia: los datos se transfieren de un nodo de la red a otro para asegurar que los datos con la misma clave estén en la misma partición.\n",
    "\n",
    "Ordenamiento: los datos se ordenan dentro de cada partición en función de la clave.\n",
    "\n",
    "El shuffling puede ser un proceso costoso, por lo que es importante minimizar su uso siempre que sea posible. Una forma de hacerlo es asegurarse de que los datos estén particionados de manera adecuada desde el principio, utilizando particionadores personalizados si es necesario. También se pueden utilizar operaciones como el reparticionamiento y la cache para optimizar el procesamiento y evitar el shuffling innecesario."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Broadcast de variables en PySpark\n",
    "El broadcasting de variables es una técnica utilizada en Spark para distribuir variables de solo lectura de manera eficiente a través de todos los nodos de un clúster de Spark. Esta técnica se utiliza para reducir el tráfico de red y mejorar el rendimiento del procesamiento de datos.\n",
    "\n",
    "Cuando una variable se transmite mediante la técnica de broadcasting, se envía solo una vez desde el nodo controlador al resto de los nodos del clúster. Luego, la variable se almacena en caché en la memoria de los nodos receptores para que pueda ser utilizada en operaciones posteriores de manera local, sin necesidad de volver a enviar la variable a través de la red. Esto reduce significativamente el tráfico de red y mejora el rendimiento del procesamiento de datos, especialmente para variables grandes que se utilizan en múltiples operaciones.\n",
    "\n",
    "Para utilizar la técnica de broadcasting de variables en Spark, se puede utilizar la función broadcast() que toma una variable de Python y devuelve una variable Broadcast de Spark que se puede utilizar en operaciones posteriores. La variable de broadcast se puede acceder en los nodos del clúster como si fuera una variable local, lo que permite una lectura eficiente y sin necesidad de transmitirla a través de la red.\n",
    "\n",
    "Es importante tener en cuenta que las variables que se transmiten mediante la técnica de broadcasting deben ser de solo lectura, ya que no se pueden modificar en los nodos del clúster. Si se necesita modificar la variable, debe transmitirse de nuevo a través de la red."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "\n",
    "# Crear una variable broadcast\n",
    "var1 = 1\n",
    "var1_broadcast = sc.broadcast(var1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T10:19:01.837644Z",
     "start_time": "2023-05-12T10:19:01.774707300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acceder a la variable broadcast\n",
    "rdd1 = rdd.map(lambda x: x + var1_broadcast.value)\n",
    "rdd1.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T10:19:11.818945300Z",
     "start_time": "2023-05-12T10:19:03.324159600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Unpersistir la variable broadcast\n",
    "var1_broadcast.unpersist()\n",
    "# Aunque se libere la variable broadcast, los nodos del clúster podrán seguir accediendo a ella, pero se volverá a transmitir a través de la red."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T10:21:49.843604Z",
     "start_time": "2023-05-12T10:21:49.801617400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o112.destroy.\n: org.apache.spark.SparkException: Attempted to use Broadcast(2) after it was destroyed (destroy at NativeMethodAccessorImpl.java:0) \r\n\tat org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144)\r\n\tat org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:106)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Destruir la variable broadcast\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mvar1_broadcast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdestroy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m rdd1 \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: x \u001B[38;5;241m+\u001B[39m var1_broadcast\u001B[38;5;241m.\u001B[39mvalue)\n\u001B[0;32m      4\u001B[0m rdd1\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[1;32mC:\\apps\\spark-3.2.4-bin-hadoop2.7\\python\\pyspark\\broadcast.py:177\u001B[0m, in \u001B[0;36mBroadcast.destroy\u001B[1;34m(self, blocking)\u001B[0m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jbroadcast \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBroadcast can only be destroyed in driver\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 177\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jbroadcast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdestroy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblocking\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m os\u001B[38;5;241m.\u001B[39munlink(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_path)\n",
      "File \u001B[1;32mC:\\apps\\spark-3.2.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[1;32mC:\\apps\\spark-3.2.4-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py:111\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 111\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m py4j\u001B[38;5;241m.\u001B[39mprotocol\u001B[38;5;241m.\u001B[39mPy4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    113\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32mC:\\apps\\spark-3.2.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o112.destroy.\n: org.apache.spark.SparkException: Attempted to use Broadcast(2) after it was destroyed (destroy at NativeMethodAccessorImpl.java:0) \r\n\tat org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144)\r\n\tat org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:106)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
     ]
    }
   ],
   "source": [
    "# Destruir la variable broadcast\n",
    "var1_broadcast.destroy()\n",
    "rdd1 = rdd.map(lambda x: x + var1_broadcast.value)  # Error al acceder a la variable broadcast porque ya no existe\n",
    "rdd1.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T10:24:54.767622500Z",
     "start_time": "2023-05-12T10:24:54.620648800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Acumuladores en PySpark\n",
    "Los acumuladores en Spark son variables que se pueden actualizar de manera segura y eficiente en paralelo durante la ejecución de tareas en un clúster. Son similares a las variables de broadcast, pero en lugar de ser sólo de lectura, se pueden agregar valores a ellas en cada tarea que se ejecuta en el clúster.\n",
    "\n",
    "Los acumuladores son útiles para realizar tareas como recuento de elementos, sumas y promedios, entre otros. En lugar de crear una variable separada para cada tarea, se puede usar un acumulador para recolectar los valores de las tareas en un solo lugar y, al final de la ejecución, obtener el resultado deseado.\n",
    "\n",
    "La actualización de los acumuladores se realiza de manera eficiente gracias a que Spark los actualiza de manera acumulativa en lugar de hacer una copia completa de la variable en cada tarea. Esto reduce la cantidad de datos que se deben transferir a través de la red y mejora el rendimiento de la aplicación.\n",
    "\n",
    "Es importante tener en cuenta que los acumuladores sólo deben ser utilizados en operaciones de sólo escritura y que su comportamiento no es determinístico en operaciones de escritura y lectura. Por lo tanto, se recomienda utilizarlos únicamente en casos en los que se esté seguro de que se van a realizar operaciones de sólo escritura."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 2, 4, 6, 8, 10]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([i for i in range(0, 11, 2)])\n",
    "rdd.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T10:37:45.892784Z",
     "start_time": "2023-05-12T10:37:45.790976600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Crear un acumulador\n",
    "accumulator = sc.accumulator(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T10:39:38.403295600Z",
     "start_time": "2023-05-12T10:39:38.372080100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "# Utilizar el acumulador\n",
    "rdd.foreach(lambda x: accumulator.add(x))\n",
    "print(accumulator.value)  # Si se ejecuta varias veces, el valor del acumulador se incrementará en cada ejecución"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T10:40:41.964919Z",
     "start_time": "2023-05-12T10:40:33.659759300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
