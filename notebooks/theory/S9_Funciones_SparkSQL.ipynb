{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_date, to_timestamp, date_format, datediff, months_between, last_day, date_add, date_sub, year, month,\n",
    "    dayofmonth, dayofyear, hour, minute, second, trim, ltrim, rtrim, rpad, lpad, concat_ws, lower, upper, initcap,\n",
    "    reverse, regexp_replace\n",
    ")\n",
    "from pyspark.sql.types import StructType, StringType, ArrayType, StructField, LongType\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T09:17:02.925302500Z",
     "start_time": "2023-05-18T09:17:02.893582200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Funciones de fecha y hora\n",
    "\n",
    "* **to_date**: Convierte una cadena en formato de fecha a un tipo de dato Date.\n",
    "* **to_timestamp**: Convierte una cadena en formato de fecha a un tipo de dato Timestamp.\n",
    "* **date_format**: Convierte una fecha a una cadena en el formato especificado.\n",
    "* **datediff**: Devuelve el número de días entre dos fechas.\n",
    "* **months_between**: Devuelve el número de meses entre dos fechas.\n",
    "* **last_day**: Devuelve el último día del mes de la fecha especificada.\n",
    "* **date_add**: Devuelve la fecha agregando el número de días especificado.\n",
    "* **date_sub**: Devuelve la fecha restando el número de días especificado.\n",
    "* **year**: Devuelve el año de la fecha especificada.\n",
    "* **month**: Devuelve el mes de la fecha especificada.\n",
    "* **dayofmonth**: Devuelve el día del mes de la fecha especificada.\n",
    "* **dayofyear**: Devuelve el día del año de la fecha especificada.\n",
    "* **hour**: Devuelve la hora de la fecha especificada.\n",
    "* **minute**: Devuelve el minuto de la fecha especificada.\n",
    "* **second**: Devuelve el segundo de la fecha especificada."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- fecha_ingreso: string (nullable = true)\n",
      " |-- fecha_salida: string (nullable = true)\n",
      " |-- baja_sistema: string (nullable = true)\n",
      "\n",
      "+------+-------------+------------+-------------------+\n",
      "|nombre|fecha_ingreso|fecha_salida|baja_sistema       |\n",
      "+------+-------------+------------+-------------------+\n",
      "|Jose  |2021-01-01   |2021-11-14  |2021-10-14 15:35:59|\n",
      "|Mayara|2021-02-06   |2021-11-25  |2021-11-25 10:35:55|\n",
      "+------+-------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.parquet(\n",
    "    r\"../../data/s9_data/calculo.parquet\"\n",
    ")\n",
    "\n",
    "data.printSchema()\n",
    "\n",
    "data.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:48:28.108483Z",
     "start_time": "2023-05-18T07:48:19.217966200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------------+\n",
      "|fecha_ingreso|fecha_salida|       baja_sistema|\n",
      "+-------------+------------+-------------------+\n",
      "|   2021-01-01|  2021-11-14|2021-10-14 15:35:59|\n",
      "|   2021-02-06|  2021-11-25|2021-11-25 10:35:55|\n",
      "+-------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to_date y to_timestamp\n",
    "\n",
    "data2 = (\n",
    "    data\n",
    "    .select(\n",
    "        to_date(col(\"fecha_ingreso\"), format=\"yyyy-MM-dd\").alias(\"fecha_ingreso\"),\n",
    "        to_date(col(\"fecha_salida\"), format=\"yyyy-MM-dd\").alias(\"fecha_salida\"),\n",
    "        to_timestamp(col(\"baja_sistema\"), format=\"yyyy-MM-dd HH:mm:ss\").alias(\"baja_sistema\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "data2.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:48:28.758832500Z",
     "start_time": "2023-05-18T07:48:28.108483Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+------------+\n",
      "|fecha_ingreso|fecha_salida|baja_sistema|\n",
      "+-------------+------------+------------+\n",
      "|   01-01-2021|  14-11-2021|  14-10-2021|\n",
      "|   06-02-2021|  25-11-2021|  25-11-2021|\n",
      "+-------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# date_format\n",
    "\n",
    "data3 = (\n",
    "    data2\n",
    "    .select(\n",
    "        date_format(\"fecha_ingreso\", format=\"dd-MM-yyyy\").alias(\"fecha_ingreso\"),\n",
    "        date_format(\"fecha_salida\", format=\"dd-MM-yyyy\").alias(\"fecha_salida\"),\n",
    "        date_format(col(\"baja_sistema\"), format=\"dd-MM-yyyy\").alias(\"baja_sistema\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "data3.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:48:29.073792Z",
     "start_time": "2023-05-18T07:48:28.758832500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+----------+\n",
      "|nombre|dias|      meses|ultimo_dia|\n",
      "+------+----+-----------+----------+\n",
      "|  Jose| 317|10.41935484|2021-11-30|\n",
      "|Mayara| 292| 9.61290323|2021-11-30|\n",
      "+------+----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datediff, months_between, last_day\n",
    "\n",
    "data4 = (\n",
    "    data\n",
    "    .select(\n",
    "        data.nombre,\n",
    "        datediff(data.fecha_salida, data.fecha_ingreso).alias(\"dias\"),\n",
    "        months_between(\"fecha_salida\", \"fecha_ingreso\").alias(\"meses\"),\n",
    "        last_day(\"fecha_salida\").alias(\"ultimo_dia\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "data4.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:48:29.455828100Z",
     "start_time": "2023-05-18T07:48:29.072778Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-----------------+----------------+\n",
      "|fecha_ingreso|fecha_salida|fecha_ingreso_+10|fecha_salida_+10|\n",
      "+-------------+------------+-----------------+----------------+\n",
      "|   2021-01-01|  2021-11-14|       2021-01-11|      2021-11-04|\n",
      "|   2021-02-06|  2021-11-25|       2021-02-16|      2021-11-15|\n",
      "+-------------+------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# date_add, date_sub\n",
    "\n",
    "data.select(\n",
    "    col(\"fecha_ingreso\"),\n",
    "    col(\"fecha_salida\"),\n",
    "    date_add(\"fecha_ingreso\", 10).alias(\"fecha_ingreso_+10\"),\n",
    "    date_sub(\"fecha_salida\", 10).alias(\"fecha_salida_+10\"),\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:48:29.677889800Z",
     "start_time": "2023-05-18T07:48:29.455828100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------------+------------+-------------+------------------+-----------------+-----------------+-------------------+-------------------+\n",
      "|fecha_ingreso|fecha_salida|       baja_sistema|year_ingreso|month_ingreso|dayofmonth_ingreso|dayofyear_ingreso|hour_baja_sistema|minute_baja_sistema|second_baja_sistema|\n",
      "+-------------+------------+-------------------+------------+-------------+------------------+-----------------+-----------------+-------------------+-------------------+\n",
      "|   2021-01-01|  2021-11-14|2021-10-14 15:35:59|        2021|            1|                 1|                1|               15|                 35|                 59|\n",
      "|   2021-02-06|  2021-11-25|2021-11-25 10:35:55|        2021|            2|                 6|               37|               10|                 35|                 55|\n",
      "+-------------+------------+-------------------+------------+-------------+------------------+-----------------+-----------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# year, month, dayofmonth, dayofyear, hour, minute, second\n",
    "\n",
    "data.select(\n",
    "    col(\"fecha_ingreso\"),\n",
    "    col(\"fecha_salida\"),\n",
    "    col(\"baja_sistema\"),\n",
    "    year(\"fecha_ingreso\").alias(\"year_ingreso\"),\n",
    "    month(\"fecha_ingreso\").alias(\"month_ingreso\"),\n",
    "    dayofmonth(\"fecha_ingreso\").alias(\"dayofmonth_ingreso\"),\n",
    "    dayofyear(\"fecha_ingreso\").alias(\"dayofyear_ingreso\"),\n",
    "    hour(\"baja_sistema\").alias(\"hour_baja_sistema\"),\n",
    "    minute(\"baja_sistema\").alias(\"minute_baja_sistema\"),\n",
    "    second(\"baja_sistema\").alias(\"second_baja_sistema\"),\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:48:30.012314200Z",
     "start_time": "2023-05-18T07:48:29.687490Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Funciones para trabajar con strings\n",
    "\n",
    "* **trim**: Elimina los espacios en blanco del inicio y final de una cadena.\n",
    "* **ltrim**: Elimina los espacios en blanco del inicio de una cadena.\n",
    "* **rtrim**: Elimina los espacios en blanco del final de una cadena.\n",
    "* **rpad**: Rellena una cadena con espacios en blanco o con el carácter especificado hasta que tenga la longitud especificada.\n",
    "* **lpad**: Rellena una cadena con espacios en blanco o con el carácter especificado hasta que tenga la longitud especificada.\n",
    "* **concat_ws**: Concatena las cadenas especificadas usando el separador especificado.\n",
    "* **lower**: Convierte una cadena a minúsculas.\n",
    "* **upper**: Convierte una cadena a mayúsculas.\n",
    "* **initcap**: Convierte la primera letra de cada palabra de una cadena a mayúsculas.\n",
    "* **reverse**: Invierte una cadena.\n",
    "* **regexp_replace**: Reemplaza todas las subcadenas que coinciden con la expresión regular especificada por la cadena especificada."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\n",
    "    r\"../../data/s9_data/data.parquet\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:48:30.131244500Z",
     "start_time": "2023-05-18T07:48:30.012314200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|nombre |\n",
      "+-------+\n",
      "| Spark |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:48:30.247434Z",
     "start_time": "2023-05-18T07:48:30.130233700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+\n",
      "| nombre| trim| ltrim| rtrim|\n",
      "+-------+-----+------+------+\n",
      "| Spark |Spark|Spark | Spark|\n",
      "+-------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trim, ltrim, rtrim\n",
    "\n",
    "df.select(\n",
    "    col(\"nombre\"),\n",
    "    trim(col(\"nombre\")).alias(\"trim\"),\n",
    "    ltrim(col(\"nombre\")).alias(\"ltrim\"),\n",
    "    rtrim(col(\"nombre\")).alias(\"rtrim\"),\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:48:30.453529Z",
     "start_time": "2023-05-18T07:48:30.247434Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+---------+---------+\n",
      "| nombre| rpad| lpad|   rpad_=|   lpad_-|\n",
      "+-------+-----+-----+---------+---------+\n",
      "| Spark |Spark|Spark|Spark ===|--- Spark|\n",
      "+-------+-----+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rpda, lpad\n",
    "\n",
    "df.select(\n",
    "    col(\"nombre\"),\n",
    "    trim(rpad(col(\"nombre\"), 10, \" \")).alias(\"rpad\"),\n",
    "    trim(lpad(col(\"nombre\"), 10, \" \")).alias(\"lpad\"),\n",
    "    trim(rpad(col(\"nombre\"), 10, \"=\")).alias(\"rpad_=\"),\n",
    "    trim(lpad(col(\"nombre\"), 10, \"-\")).alias(\"lpad_-\"),\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:48:30.812610800Z",
     "start_time": "2023-05-18T07:48:30.435759500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+\n",
      "|sujeto|verbo|adjetivo|\n",
      "+------+-----+--------+\n",
      "| spark|   es|  genial|\n",
      "+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    data=[\n",
    "        (\"spark\", \"es\", \"genial\")\n",
    "    ],\n",
    "    schema=[\"sujeto\", \"verbo\", \"adjetivo\"])\n",
    "\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:48:58.367675900Z",
     "start_time": "2023-05-18T07:48:30.781334700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----+-------+-------+\n",
      "|      concat_ws|lower|upper|initcap|reverse|\n",
      "+---------------+-----+-----+-------+-------+\n",
      "|spark es genial|spark|SPARK|  Spark|  kraps|\n",
      "+---------------+-----+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# concat_ws, lower, upper, initcap, reverse\n",
    "\n",
    "df.select(\n",
    "    concat_ws(\" \", col(\"sujeto\"), col(\"verbo\"), col(\"adjetivo\")).alias(\"concat_ws\"),\n",
    "    lower(col(\"sujeto\")).alias(\"lower\"),\n",
    "    upper(col(\"sujeto\")).alias(\"upper\"),\n",
    "    initcap(col(\"sujeto\")).alias(\"initcap\"),\n",
    "    reverse(col(\"sujeto\")).alias(\"reverse\"),\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:49:07.143755Z",
     "start_time": "2023-05-18T07:48:58.371189800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|frase                       |\n",
      "+----------------------------+\n",
      "| voy a casa a por mis llaves|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regexp_replace\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    data=[\n",
    "        (\" voy a casa a por mis llaves\",)\n",
    "    ],\n",
    "    schema=[\"frase\"])\n",
    "\n",
    "df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:49:15.457089Z",
     "start_time": "2023-05-18T07:49:07.147371200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------+\n",
      "|frase                       |regexp_replace            |\n",
      "+----------------------------+--------------------------+\n",
      "| voy a casa a por mis llaves| ir a casa a ir mis llaves|\n",
      "+----------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regexp_replace\n",
    "\n",
    "df.select(\n",
    "    col(\"frase\"),\n",
    "    regexp_replace(col(\"frase\"), \"voy|por\", \"ir\").alias(\"regexp_replace\"),\n",
    ").show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T07:49:23.640802Z",
     "start_time": "2023-05-18T07:49:15.450964500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Funciones para trabajar con colecciones\n",
    "\n",
    "* **size**: Devuelve el tamaño de la colección.\n",
    "\n",
    "* **sort_array**: Ordena los elementos de la colección en orden ascendente o descendente según el parámetro especificado. Si no se especifica el parámetro, se ordena en orden ascendente. Los elementos han de ser del mismo tipo.\n",
    "\n",
    "* **array_contains**: Devuelve true si el array contiene el valor especificado.\n",
    "\n",
    "* **explode**: Crea una fila por cada elemento de la colección.\n",
    "\n",
    "* **to_json**: Se utiliza para convertir una columna o estructura de datos en formato JSON. Toma una columna o una expresión de estructura y devuelve una cadena de caracteres en formato JSON que representa los datos. Esto es útil cuando quieres convertir datos estructurados en un formato que sea compatible con JSON.\n",
    "\n",
    "* **from_json**: Se utiliza para convertir una columna de tipo JSON en una estructura de datos anidada de PySpark. Toma una columna JSON y un esquema (schema) de PySpark como argumentos y devuelve una nueva columna que contiene la estructura de datos anidada. Esta función es útil cuando quieres extraer datos de una cadena JSON y utilizarlos en consultas o transformaciones posteriores.\n",
    "\n",
    "* **get_json_object**: Se utiliza para extraer un valor específico de una cadena JSON. Toma una columna JSON y una ruta (path) en formato de cadena como argumentos, y devuelve el valor correspondiente a esa ruta en la cadena JSON. Puedes utilizar esta función para acceder a valores específicos dentro de una estructura de datos JSON sin tener que convertir toda la cadena JSON en una estructura anidada."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size, sort_array, array_contains, explode"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:05:35.858287500Z",
     "start_time": "2023-05-18T08:05:35.795243200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------+\n",
      "|dia  |tareas                                      |\n",
      "+-----+--------------------------------------------+\n",
      "|lunes|[hacer la tarea, buscar agua, lavar el auto]|\n",
      "+-----+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\n",
    "    r\"../../data/s9_data/parquet\"\n",
    ")\n",
    "\n",
    "df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:05:49.790307200Z",
     "start_time": "2023-05-18T08:05:41.672618400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|dia  |size(tareas)|\n",
      "+-----+------------+\n",
      "|lunes|3           |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# size\n",
    "\n",
    "df.select(\n",
    "    col(\"dia\"),\n",
    "    size(col(\"tareas\")),\n",
    ").show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:05:50.088491200Z",
     "start_time": "2023-05-18T08:05:49.790307200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------+\n",
      "|dia  |tareas_ordenadas                            |\n",
      "+-----+--------------------------------------------+\n",
      "|lunes|[buscar agua, hacer la tarea, lavar el auto]|\n",
      "+-----+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort_array\n",
    "\n",
    "df.select(\n",
    "    col(\"dia\"),\n",
    "    sort_array(col(\"tareas\")).alias(\"tareas_ordenadas\"),\n",
    ").show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:05:50.505626Z",
     "start_time": "2023-05-18T08:05:50.072842800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|dia  |limpiar|\n",
      "+-----+-------+\n",
      "|lunes|false  |\n",
      "+-----+-------+\n",
      "\n",
      "+-----+--------------------------------------------+-------------+\n",
      "|dia  |tareas                                      |lavar el auto|\n",
      "+-----+--------------------------------------------+-------------+\n",
      "|lunes|[hacer la tarea, buscar agua, lavar el auto]|true         |\n",
      "+-----+--------------------------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array_contains\n",
    "\n",
    "df.select(\n",
    "    col(\"dia\"),\n",
    "    array_contains(col(\"tareas\"), \"limpiar\").alias(\"limpiar\"),\n",
    ").show(truncate=False)\n",
    "\n",
    "df.select(\n",
    "    col(\"dia\"),\n",
    "    col(\"tareas\"),\n",
    "    array_contains(col(\"tareas\"), \"lavar el auto\").alias(\"lavar el auto\"),\n",
    ").show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:05:51.088919200Z",
     "start_time": "2023-05-18T08:05:50.505626Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|dia  |tarea         |\n",
      "+-----+--------------+\n",
      "|lunes|hacer la tarea|\n",
      "|lunes|buscar agua   |\n",
      "|lunes|lavar el auto |\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode\n",
    "\n",
    "df.select(\n",
    "    col(\"dia\"),\n",
    "    explode(col(\"tareas\")).alias(\"tarea\"),\n",
    ").show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:08:06.557192800Z",
     "start_time": "2023-05-18T08:08:06.360298600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|tareas_str                                                                 |\n",
      "+---------------------------------------------------------------------------+\n",
      "|{\"dia\": \"lunes\",\"tareas\": [\"hacer la tarea\",\"buscar agua\",\"lavar el auto\"]}|\n",
      "+---------------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- tareas_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JSON\n",
    "\n",
    "from pyspark.sql.functions import from_json, get_json_object, to_json\n",
    "\n",
    "df = spark.read.parquet(\n",
    "    r\"../../data/s9_data/JSON\"\n",
    ")\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:11:20.164003300Z",
     "start_time": "2023-05-18T08:11:19.922274500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tareas_json: struct (nullable = true)\n",
      " |    |-- dia: string (nullable = true)\n",
      " |    |-- tareas: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n",
      "+-----------------------------------------------------+\n",
      "|tareas_json                                          |\n",
      "+-----------------------------------------------------+\n",
      "|{lunes, [hacer la tarea, buscar agua, lavar el auto]}|\n",
      "+-----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from_json\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"dia\", StringType(), nullable=True),\n",
    "    StructField(\"tareas\", ArrayType(StringType()), nullable=True),\n",
    "])\n",
    "\n",
    "json_df = df.select(\n",
    "    from_json(col(\"tareas_str\"), schema=schema).alias(\"tareas_json\"),\n",
    ")\n",
    "\n",
    "json_df.printSchema()\n",
    "\n",
    "json_df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:22:17.745256200Z",
     "start_time": "2023-05-18T08:22:17.442892900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------+--------------+-----------+-------------+\n",
      "|dia  |tareas                                      |tarea_0       |tarea_1    |tarea_2      |\n",
      "+-----+--------------------------------------------+--------------+-----------+-------------+\n",
      "|lunes|[hacer la tarea, buscar agua, lavar el auto]|hacer la tarea|buscar agua|lavar el auto|\n",
      "+-----+--------------------------------------------+--------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(\n",
    "    col(\"tareas_json\").getItem(\"dia\").alias(\"dia\"),\n",
    "    col(\"tareas_json\").getItem(\"tareas\").alias(\"tareas\"),\n",
    "    col(\"tareas_json\").getItem(\"tareas\").getItem(0).alias(\"tarea_0\"),\n",
    "    col(\"tareas_json\").getItem(\"tareas\").getItem(1).alias(\"tarea_1\"),\n",
    "    col(\"tareas_json\").getItem(\"tareas\").getItem(2).alias(\"tarea_2\"),\n",
    ").show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:26:02.347178600Z",
     "start_time": "2023-05-18T08:26:02.121268100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+\n",
      "|tareas_json                                          |\n",
      "+-----------------------------------------------------+\n",
      "|{lunes, [hacer la tarea, buscar agua, lavar el auto]}|\n",
      "+-----------------------------------------------------+\n",
      "\n",
      "+-------------------------------------------------------------------------+\n",
      "|tareas_str                                                               |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"dia\":\"lunes\",\"tareas\":[\"hacer la tarea\",\"buscar agua\",\"lavar el auto\"]}|\n",
      "+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to_json\n",
    "\n",
    "json_df.show(truncate=False)\n",
    "\n",
    "to_json_df = json_df.select(\n",
    "    to_json(col(\"tareas_json\")).alias(\"tareas_str\"),\n",
    ")\n",
    "\n",
    "to_json_df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:28:49.428678300Z",
     "start_time": "2023-05-18T08:28:49.222080500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------+\n",
      "|dia  |tareas                                          |\n",
      "+-----+------------------------------------------------+\n",
      "|lunes|[\"hacer la tarea\",\"buscar agua\",\"lavar el auto\"]|\n",
      "+-----+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get_json_object\n",
    "\n",
    "to_json_df.select(\n",
    "    get_json_object(col(\"tareas_str\"), \"$.dia\").alias(\"dia\"),\n",
    "    get_json_object(col(\"tareas_str\"), \"$.tareas\").alias(\"tareas\"),\n",
    ").show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:28:51.132406300Z",
     "start_time": "2023-05-18T08:28:50.865448300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Funciones when, coalesce y lit\n",
    "\n",
    "* **when**: Se utiliza para aplicar condiciones lógicas a los datos. Toma una expresión booleana y un valor o una columna como argumentos. Si la expresión booleana se evalúa como verdadera, devuelve el valor o la columna especificada. Puedes usar when en combinación con otras funciones para aplicar transformaciones condicionales en tus datos.\n",
    "\n",
    "* **coalesce**: Se utiliza para obtener el primer valor no nulo de una lista de columnas. Toma una lista de columnas y devuelve una nueva columna que contiene el primer valor no nulo de esa lista. Es útil cuando quieres obtener un valor de respaldo en caso de que una columna tenga valores nulos.\n",
    "\n",
    "* **lit**: Se utiliza para crear una columna constante con un valor especificado. Toma un valor como argumento y devuelve una nueva columna que contiene ese valor constante. Puedes usar lit para agregar columnas con valores constantes a tus datos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- pago: long (nullable = true)\n",
      "\n",
      "+------+----+\n",
      "|nombre|pago|\n",
      "+------+----+\n",
      "|Jose  |1   |\n",
      "|Julia |2   |\n",
      "|Katia |1   |\n",
      "|null  |3   |\n",
      "|Raul  |3   |\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when, coalesce, lit\n",
    "\n",
    "from pyspark.sql.functions import when, coalesce, lit\n",
    "\n",
    "df = spark.read.parquet(\n",
    "    r\"../../data/s9_data/l83\"\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:39:13.045331100Z",
     "start_time": "2023-05-18T08:39:12.771176900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+\n",
      "|nombre|pago|nivel_pago|\n",
      "+------+----+----------+\n",
      "|Jose  |1   |bajo      |\n",
      "|Julia |2   |medio     |\n",
      "|Katia |1   |bajo      |\n",
      "|null  |3   |alto      |\n",
      "|Raul  |3   |alto      |\n",
      "+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when pago > 1 alto, when pago > 1 and pago < 2 medio, when pago < 1 bajo\n",
    "\n",
    "df.select(\n",
    "    col(\"nombre\"),\n",
    "    col(\"pago\"),\n",
    "    when(col(\"pago\") >= 3, \"alto\").when((col(\"pago\") > 1) & (col(\"pago\") < 3), \"medio\").otherwise(\"bajo\").alias(\n",
    "        \"nivel_pago\"),\n",
    ").show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:54:05.126954Z",
     "start_time": "2023-05-18T08:54:04.965303200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------------+\n",
      "|nombre|pago|nombre_no_nulo|\n",
      "+------+----+--------------+\n",
      "|Jose  |1   |Jose          |\n",
      "|Julia |2   |Julia         |\n",
      "|Katia |1   |Katia         |\n",
      "|null  |3   |sin nombre    |\n",
      "|Raul  |3   |Raul          |\n",
      "+------+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# coalesce y lit\n",
    "\n",
    "df.select(\n",
    "    col(\"nombre\"),\n",
    "    col(\"pago\"),\n",
    "    coalesce(col(\"nombre\"), lit(\"sin nombre\")).alias(\"nombre_no_nulo\"),\n",
    ").show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T08:57:31.607037800Z",
     "start_time": "2023-05-18T08:57:31.481829Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Funciones definidas por el usuario (UDF)\n",
    "\n",
    "Las User-Defined Functions (UDF) son funciones personalizadas definidas por el usuario que se pueden utilizar para realizar transformaciones complejas en los datos. Estas funciones permiten extender las funcionalidades de PySpark más allá de las funciones integradas y proporcionan flexibilidad para aplicar lógica personalizada a tus datos.\n",
    "\n",
    "Aquí hay algunos puntos clave sobre las UDF en PySpark:\n",
    "\n",
    "* **Definición**: Una UDF se define utilizando una función de Python regular. Puedes escribir la lógica personalizada dentro de esta función para procesar los datos según tus necesidades.\n",
    "\n",
    "* **Registro de la UDF**: Después de definir una UDF, debes registrarla con PySpark para que pueda reconocerla y aplicarla a los datos. Para registrar una UDF, utilizas el método udf del módulo pyspark.sql.functions. Este método toma como argumento la función definida por el usuario y devuelve una UDF registrada.\n",
    "\n",
    "* **Tipos de UDF**: Las UDF pueden ser clasificadas en dos categorías principales:\n",
    "\n",
    "    * **UDF escalar**: Toma una o más columnas como entrada y devuelve un solo valor como resultado. Puedes aplicar una UDF escalar a una columna o a un conjunto de columnas para generar una nueva columna con los resultados de la función personalizada.\n",
    "\n",
    "    * **UDF de columna**: Toma una o más columnas como entrada y devuelve una nueva columna como resultado. La UDF de columna se aplica a nivel de fila y puedes utilizarla para aplicar transformaciones más complejas que involucren múltiples columnas.\n",
    "\n",
    "* **Aplicación de UDF**: Una vez registrada, puedes aplicar la UDF a tus datos utilizando las funciones de PySpark, como select, withColumn, groupBy, etc. Puedes llamar a la UDF dentro de estas funciones y pasar las columnas requeridas como argumentos.\n",
    "\n",
    "Es importante tener en cuenta que el uso excesivo de UDF puede afectar el rendimiento, ya que las UDF implican la ejecución de código Python a nivel de fila en lugar de operaciones vectorizadas optimizadas. Por lo tanto, se recomienda utilizar las funciones integradas de PySpark siempre que sea posible y utilizar UDF solo cuando sea necesario para casos de lógica personalizada compleja."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.cubed(n: int) -> int>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cubed(n: int) -> int:\n",
    "    return n * n * n\n",
    "\n",
    "\n",
    "spark.udf.register(\"cubed\", cubed, LongType())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T09:11:02.192566300Z",
     "start_time": "2023-05-18T09:11:02.086839100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "spark.range(1, 100).createOrReplaceTempView(\"test\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T09:11:30.171639500Z",
     "start_time": "2023-05-18T09:11:30.003154100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "|  9|     729|\n",
      "| 10|    1000|\n",
      "| 11|    1331|\n",
      "| 12|    1728|\n",
      "| 13|    2197|\n",
      "| 14|    2744|\n",
      "| 15|    3375|\n",
      "| 16|    4096|\n",
      "| 17|    4913|\n",
      "| 18|    5832|\n",
      "| 19|    6859|\n",
      "| 20|    8000|\n",
      "+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM test\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T09:13:18.045573700Z",
     "start_time": "2023-05-18T09:13:14.892043900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# def welcome(name: str) -> str:\n",
    "#     return f\"Welcome to PySpark {name}!\"\n",
    "\n",
    "welcome_udf = udf(lambda name: f\"Welcome {name}!\", StringType())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T09:21:56.630352Z",
     "start_time": "2023-05-18T09:21:56.594611300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "| name|       welcome|\n",
      "+-----+--------------+\n",
      "| John| Welcome John!|\n",
      "|James|Welcome James!|\n",
      "| Anna| Welcome Anna!|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_names = spark.createDataFrame(\n",
    "    data=[(\"John\",), (\"James\",), (\"Anna\",)],\n",
    "    schema=[\"name\"]\n",
    ")\n",
    "\n",
    "df_names.select(\n",
    "    col(\"name\"),\n",
    "    welcome_udf(col(\"name\")).alias(\"welcome\")\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T09:21:39.716704400Z",
     "start_time": "2023-05-18T09:21:22.782221500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|name |welcome|\n",
      "+-----+-------+\n",
      "|John |JOHN   |\n",
      "|James|JAMES  |\n",
      "|Anna |ANNA   |\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf(returnType=StringType())\n",
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "df_names.select(\n",
    "    col(\"name\"),\n",
    "    upper(col(\"name\")).alias(\"welcome\")\n",
    ").show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T09:27:16.157656200Z",
     "start_time": "2023-05-18T09:26:59.234834700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UDF vectorizadas o Pandas UDF\n",
    "\n",
    "Las UDF vectorizadas o Pandas UDF (User-Defined Functions) son una extensión de las UDF en PySpark que permiten procesar datos de forma eficiente al aplicar operaciones en lotes utilizando la biblioteca Pandas.\n",
    "\n",
    "En PySpark, las UDF vectorizadas aprovechan la capacidad de Pandas para procesar datos en lotes, lo que implica una ejecución más rápida en comparación con las UDF regulares que operan a nivel de fila. En lugar de procesar los datos fila por fila, las UDF vectorizadas procesan los datos en grupos (batches) y aprovechan las operaciones vectorizadas optimizadas de Pandas.\n",
    "\n",
    "Aquí hay algunos puntos clave sobre las UDF vectorizadas o Pandas UDF:\n",
    "\n",
    "* **Funcionamiento**: Las UDF vectorizadas toman como entrada un DataFrame de PySpark y aplican operaciones en lotes a los datos utilizando Pandas. El DataFrame de PySpark se divide en lotes más pequeños y cada lote se convierte en un DataFrame de Pandas para aplicar la lógica personalizada utilizando las capacidades vectorizadas de Pandas. Luego, los resultados se devuelven como un nuevo DataFrame de PySpark.\n",
    "\n",
    "* **Eficiencia**: Al procesar datos en lotes utilizando Pandas, las UDF vectorizadas reducen la sobrecarga de comunicación entre Python y el entorno de ejecución de PySpark, lo que puede conducir a una mejora significativa en el rendimiento y la eficiencia en comparación con las UDF regulares. Además, Pandas ofrece una amplia gama de funciones y operaciones optimizadas para el procesamiento de datos, lo que puede acelerar aún más las transformaciones.\n",
    "\n",
    "* **Tipos de UDF vectorizadas**: Al igual que las UDF regulares, las UDF vectorizadas también pueden ser UDF escalares o UDF de columna. Puedes aplicar una UDF vectorizada escalar a una o más columnas y generar una nueva columna con los resultados vectorizados. Por otro lado, una UDF vectorizada de columna toma un conjunto de columnas como entrada y devuelve un nuevo DataFrame con las columnas resultantes.\n",
    "\n",
    "* **Restricciones**: Las UDF vectorizadas tienen algunas restricciones en cuanto a las operaciones compatibles y los tipos de datos. Al utilizar Pandas, las operaciones deben ser compatibles con el modelo de datos de Pandas y no se admiten todas las funciones y operaciones de PySpark. Además, las UDF vectorizadas no son adecuadas para todas las situaciones y pueden requerir más memoria, especialmente cuando se trabajan con grandes volúmenes de datos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.pandas_cubed_udf(v: pandas.core.series.Series) -> pandas.core.series.Series>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "def pandas_cubed_udf(v: pd.Series) -> pd.Series:\n",
    "    return v * v * v\n",
    "\n",
    "pandas_udf(pandas_cubed_udf, returnType=LongType())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T09:42:08.081971900Z",
     "start_time": "2023-05-18T09:42:08.042059500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "pd_series = pd.Series([1, 2, 3, 4, 5])\n",
    "\n",
    "pd_series = pandas_cubed_udf(pd_series)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T09:46:53.560023900Z",
     "start_time": "2023-05-18T09:46:53.497207900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|num|letter|\n",
      "+---+------+\n",
      "|1  |a     |\n",
      "|2  |b     |\n",
      "|3  |c     |\n",
      "+---+------+\n",
      "\n",
      "+---------+\n",
      "|num_cubed|\n",
      "+---------+\n",
      "|1        |\n",
      "|8        |\n",
      "|27       |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)\n",
    "\n",
    "df.select(\n",
    "    pandas_cubed_udf(col(\"num\")).alias(\"num_cubed\")\n",
    ").show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T09:47:48.074987500Z",
     "start_time": "2023-05-18T09:47:30.594519800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Funciones de ventana (Window Functions) en PySpark\n",
    "\n",
    "Las funciones de ventana (window functions) en PySpark son operaciones analíticas avanzadas que se aplican a un conjunto de filas en una ventana definida. Estas funciones permiten realizar cálculos y transformaciones basados en grupos de filas en lugar de operar en filas individuales, lo que facilita el análisis y la agregación de datos en ventanas específicas.\n",
    "\n",
    "Aquí hay algunos puntos clave sobre las funciones de ventana en PySpark:\n",
    "\n",
    "* **Definición de ventana**: Antes de aplicar una función de ventana, debes definir la ventana utilizando la cláusula window en combinación con las funciones de agregación o análisis. Una ventana se define mediante una especificación de partición (partition), ordenación (ordering) y un marco (frame). La especificación de partición divide los datos en grupos lógicos, la ordenación establece el orden de las filas dentro de cada grupo y el marco define las filas relativas utilizadas en los cálculos.\n",
    "\n",
    "* **Funciones de ventana comunes**: PySpark proporciona una variedad de funciones de ventana integradas para realizar cálculos y transformaciones. Algunas funciones comunes incluyen rank, row_number, dense_rank, lag, lead, sum, avg, min, max, first_value y last_value. Estas funciones se aplican a las filas dentro de la ventana y generan resultados basados en la lógica definida.\n",
    "\n",
    "* **Uso de funciones de ventana**: Las funciones de ventana se utilizan en combinación con la función over, que especifica la ventana a la que se aplicará la función. La función over se aplica a una columna y toma como argumento la definición de la ventana que se ha creado previamente. Puedes utilizar la función over junto con una función de ventana para generar una nueva columna con los resultados calculados.\n",
    "\n",
    "* **Tipos de ventanas**: PySpark admite varios tipos de ventanas, como ventanas sin partición (unbounded), ventanas particionadas (partitioned), ventanas deslizantes (sliding), ventanas de rango (range) y ventanas de fila (row). Cada tipo de ventana tiene su propia lógica y sintaxis para definir la ventana y realizar los cálculos correspondientes.\n",
    "\n",
    "## Funciones\n",
    "\n",
    "* **row_number**: Asigna un número de fila a cada fila en una partición.\n",
    "* **rank**: Asigna un rango a cada fila en una partición, pero si hay empates, los números de fila no son consecutivos.\n",
    "* **dense_rank**: Asigna un rango a cada fila en una partición, pero si hay empates, los números de fila son consecutivos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- edad: integer (nullable = true)\n",
      " |-- departamento: string (nullable = true)\n",
      " |-- evaluacion: integer (nullable = true)\n",
      "\n",
      "+-------+----+------------+----------+\n",
      "|nombre |edad|departamento|evaluacion|\n",
      "+-------+----+------------+----------+\n",
      "|Lazaro |45  |letras      |98        |\n",
      "|Raul   |24  |matemática  |76        |\n",
      "|Maria  |34  |matemática  |27        |\n",
      "|Jose   |30  |química     |78        |\n",
      "|Susana |51  |química     |98        |\n",
      "|Juan   |44  |letras      |89        |\n",
      "|Julia  |55  |letras      |92        |\n",
      "|Kadir  |38  |arquitectura|39        |\n",
      "|Lilian |23  |arquitectura|94        |\n",
      "|Rosa   |26  |letras      |91        |\n",
      "|Aian   |50  |matemática  |73        |\n",
      "|Yaneisy|29  |letras      |89        |\n",
      "|Enrique|40  |química     |92        |\n",
      "|Jon    |25  |arquitectura|78        |\n",
      "|Luisa  |39  |arquitectura|94        |\n",
      "+-------+----+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\n",
    "    r\"../../data/s10/funciones_ventana.parquet\"\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T09:53:43.982758100Z",
     "start_time": "2023-05-18T09:53:43.748836400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, desc\n",
    "\n",
    "window_spec = Window.partitionBy(\"departamento\").orderBy(desc(df.evaluacion))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T10:11:03.609711400Z",
     "start_time": "2023-05-18T10:11:03.581033900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----------+\n",
      "|nombre |edad|departamento|evaluacion|row_number|\n",
      "+-------+----+------------+----------+----------+\n",
      "|Lilian |23  |arquitectura|94        |1         |\n",
      "|Luisa  |39  |arquitectura|94        |2         |\n",
      "|Lazaro |45  |letras      |98        |1         |\n",
      "|Julia  |55  |letras      |92        |2         |\n",
      "|Raul   |24  |matemática  |76        |1         |\n",
      "|Aian   |50  |matemática  |73        |2         |\n",
      "|Susana |51  |química     |98        |1         |\n",
      "|Enrique|40  |química     |92        |2         |\n",
      "+-------+----+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# row_number\n",
    "\n",
    "df.withColumn(\n",
    "    \"row_number\",\n",
    "    row_number().over(window_spec)\n",
    ").filter(col(\"row_number\") <= 2).show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T10:32:57.026795100Z",
     "start_time": "2023-05-18T10:32:56.711929100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----+\n",
      "|nombre |edad|departamento|evaluacion|rank|\n",
      "+-------+----+------------+----------+----+\n",
      "|Lilian |23  |arquitectura|94        |1   |\n",
      "|Luisa  |39  |arquitectura|94        |1   |\n",
      "|Lazaro |45  |letras      |98        |1   |\n",
      "|Julia  |55  |letras      |92        |2   |\n",
      "|Raul   |24  |matemática  |76        |1   |\n",
      "|Aian   |50  |matemática  |73        |2   |\n",
      "|Susana |51  |química     |98        |1   |\n",
      "|Enrique|40  |química     |92        |2   |\n",
      "+-------+----+------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rank\n",
    "\n",
    "df.withColumn(\n",
    "    \"rank\",\n",
    "    rank().over(window_spec)\n",
    ").filter(col(\"rank\") <= 2).show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T10:35:23.498849500Z",
     "start_time": "2023-05-18T10:35:22.808885100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----------+\n",
      "|nombre |edad|departamento|evaluacion|dense_rank|\n",
      "+-------+----+------------+----------+----------+\n",
      "|Lilian |23  |arquitectura|94        |1         |\n",
      "|Luisa  |39  |arquitectura|94        |1         |\n",
      "|Jon    |25  |arquitectura|78        |2         |\n",
      "|Lazaro |45  |letras      |98        |1         |\n",
      "|Julia  |55  |letras      |92        |2         |\n",
      "|Raul   |24  |matemática  |76        |1         |\n",
      "|Aian   |50  |matemática  |73        |2         |\n",
      "|Susana |51  |química     |98        |1         |\n",
      "|Enrique|40  |química     |92        |2         |\n",
      "+-------+----+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dense_rank\n",
    "\n",
    "df.withColumn(\n",
    "    \"dense_rank\",\n",
    "    dense_rank().over(window_spec)\n",
    ").filter(col(\"dense_rank\") <= 2).show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T10:37:31.450865600Z",
     "start_time": "2023-05-18T10:37:31.235744300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+--------------+--------------+------------------+----------+\n",
      "|nombre |edad|departamento|evaluacion|min_evaluacion|max_evaluacion|avg_evaluacion    |row_number|\n",
      "+-------+----+------------+----------+--------------+--------------+------------------+----------+\n",
      "|Lilian |23  |arquitectura|94        |39            |94            |76.25             |1         |\n",
      "|Luisa  |39  |arquitectura|94        |39            |94            |76.25             |2         |\n",
      "|Jon    |25  |arquitectura|78        |39            |94            |76.25             |3         |\n",
      "|Kadir  |38  |arquitectura|39        |39            |94            |76.25             |4         |\n",
      "|Lazaro |45  |letras      |98        |89            |98            |91.8              |1         |\n",
      "|Julia  |55  |letras      |92        |89            |98            |91.8              |2         |\n",
      "|Rosa   |26  |letras      |91        |89            |98            |91.8              |3         |\n",
      "|Juan   |44  |letras      |89        |89            |98            |91.8              |4         |\n",
      "|Yaneisy|29  |letras      |89        |89            |98            |91.8              |5         |\n",
      "|Raul   |24  |matemática  |76        |27            |76            |58.666666666666664|1         |\n",
      "|Aian   |50  |matemática  |73        |27            |76            |58.666666666666664|2         |\n",
      "|Maria  |34  |matemática  |27        |27            |76            |58.666666666666664|3         |\n",
      "|Susana |51  |química     |98        |78            |98            |89.33333333333333 |1         |\n",
      "|Enrique|40  |química     |92        |78            |98            |89.33333333333333 |2         |\n",
      "|Jose   |30  |química     |78        |78            |98            |89.33333333333333 |3         |\n",
      "+-------+----+------------+----------+--------------+--------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, avg\n",
    "\n",
    "window_spec_agg = Window.partitionBy(\"departamento\")\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\n",
    "        \"min_evaluacion\",\n",
    "        min(df.evaluacion).over(window_spec_agg),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"max_evaluacion\",\n",
    "        max(df.evaluacion).over(window_spec_agg),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"avg_evaluacion\",\n",
    "        avg(df.evaluacion).over(window_spec_agg),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"row_number\",\n",
    "        row_number().over(window_spec),\n",
    "    )\n",
    "    .show(truncate=False)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T10:44:16.146166900Z",
     "start_time": "2023-05-18T10:44:15.876741800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Catalyst optimizer\n",
    "\n",
    "Catalyst Optimizer es un componente fundamental en Apache Spark y, por lo tanto, también en PySpark. Es un sistema de optimización de consultas basado en reglas que se encarga de analizar, transformar y optimizar el plan de ejecución de las consultas en Spark. Su objetivo es generar un plan de ejecución eficiente que minimice el tiempo de procesamiento y utilice de manera óptima los recursos disponibles.\n",
    "\n",
    "A continuación, se presentan algunas características clave del Catalyst Optimizer en PySpark:\n",
    "\n",
    "* **Análisis y optimización lógica**: El Catalyst Optimizer realiza un análisis exhaustivo de la estructura lógica de las consultas y aplica diversas optimizaciones basadas en reglas. Examina la estructura de las expresiones, las relaciones entre tablas y las operaciones a realizar, buscando formas más eficientes de ejecutar la consulta.\n",
    "\n",
    "* **Árbol de expresiones**: El Catalyst Optimizer utiliza un árbol de expresiones para representar las operaciones lógicas y físicas de una consulta. Este árbol permite una manipulación flexible y modular de las expresiones y las operaciones, lo que facilita la aplicación de reglas de optimización y la transformación del plan de ejecución.\n",
    "\n",
    "* **Reglas de optimización**: Catalyst utiliza un conjunto de reglas de optimización predefinidas para reescribir y mejorar el plan de ejecución de la consulta. Estas reglas incluyen fusionar operaciones redundantes, eliminar operaciones innecesarias, cambiar el orden de ejecución de las operaciones y aplicar técnicas como la poda de columnas (column pruning) y la poda de filtros (filter pushdown) para reducir el tamaño de los datos a procesar.\n",
    "\n",
    "* **Optimización física**: Además de las optimizaciones lógicas, el Catalyst Optimizer también realiza optimizaciones físicas. Determina la distribución de datos más eficiente para las operaciones y selecciona los algoritmos de ejecución más adecuados, como la selección de un join hash o un join de bucle anidado en función de las características de los datos y los recursos disponibles.\n",
    "\n",
    "* **Extensibilidad**: El Catalyst Optimizer en PySpark es altamente extensible. Permite a los desarrolladores y usuarios agregar nuevas reglas de optimización personalizadas o modificar las reglas existentes para adaptarse a casos de uso específicos. Esto proporciona flexibilidad para ajustar y optimizar el plan de ejecución según los requisitos y las características de los datos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\n",
    "    r\"../../data/s10/vuelos.parquet\"\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T11:00:34.385541500Z",
     "start_time": "2023-05-18T11:00:34.238033100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|AIRLINE|DIS_AIRTIME      |\n",
      "+-------+-----------------+\n",
      "|AA     |7.649769585253456|\n",
      "|DL     |8.14176245210728 |\n",
      "|DL     |8.361344537815127|\n",
      "|DL     |8.358851674641148|\n",
      "|UA     |8.147826086956522|\n",
      "|UA     |8.265402843601896|\n",
      "|AA     |8.610294117647058|\n",
      "|AS     |7.413461538461538|\n",
      "|AA     |8.5              |\n",
      "|DL     |8.10135135135135 |\n",
      "|DL     |8.779005524861878|\n",
      "|DL     |8.08955223880597 |\n",
      "|UA     |8.146666666666667|\n",
      "|UA     |8.39090909090909 |\n",
      "|AS     |8.274038461538462|\n",
      "|DL     |8.53048780487805 |\n",
      "|DL     |8.11111111111111 |\n",
      "|UA     |8.336787564766839|\n",
      "|DL     |8.434065934065934|\n",
      "|UA     |8.088372093023256|\n",
      "+-------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = (\n",
    "    df\n",
    "    .filter(\n",
    "        col(\"MONTH\").isin(6, 7, 8),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"DIS_AIRTIME\",\n",
    "        col(\"DISTANCE\") / col(\"AIR_TIME\"),\n",
    "    )\n",
    "    .select(\n",
    "        col(\"AIRLINE\"),\n",
    "        col(\"DIS_AIRTIME\"),\n",
    "    )\n",
    "    .where(\n",
    "        col(\"AIRLINE\").isin(\"AA\", \"AS\", \"DL\", \"UA\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "new_df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T11:05:04.683054600Z",
     "start_time": "2023-05-18T11:05:03.822536100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter 'AIRLINE IN (AA,AS,DL,UA)\n",
      "+- Project [AIRLINE#1562, DIS_AIRTIME#1620]\n",
      "   +- Project [YEAR#1558, MONTH#1559, DAY#1560, DAY_OF_WEEK#1561, AIRLINE#1562, FLIGHT_NUMBER#1563, TAIL_NUMBER#1564, ORIGIN_AIRPORT#1565, DESTINATION_AIRPORT#1566, SCHEDULED_DEPARTURE#1567, DEPARTURE_TIME#1568, DEPARTURE_DELAY#1569, TAXI_OUT#1570, WHEELS_OFF#1571, SCHEDULED_TIME#1572, ELAPSED_TIME#1573, AIR_TIME#1574, DISTANCE#1575, WHEELS_ON#1576, TAXI_IN#1577, SCHEDULED_ARRIVAL#1578, ARRIVAL_TIME#1579, ARRIVAL_DELAY#1580, DIVERTED#1581, ... 8 more fields]\n",
      "      +- Filter MONTH#1559 IN (6,7,8)\n",
      "         +- Relation [YEAR#1558,MONTH#1559,DAY#1560,DAY_OF_WEEK#1561,AIRLINE#1562,FLIGHT_NUMBER#1563,TAIL_NUMBER#1564,ORIGIN_AIRPORT#1565,DESTINATION_AIRPORT#1566,SCHEDULED_DEPARTURE#1567,DEPARTURE_TIME#1568,DEPARTURE_DELAY#1569,TAXI_OUT#1570,WHEELS_OFF#1571,SCHEDULED_TIME#1572,ELAPSED_TIME#1573,AIR_TIME#1574,DISTANCE#1575,WHEELS_ON#1576,TAXI_IN#1577,SCHEDULED_ARRIVAL#1578,ARRIVAL_TIME#1579,ARRIVAL_DELAY#1580,DIVERTED#1581,... 7 more fields] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "AIRLINE: string, DIS_AIRTIME: double\n",
      "Filter AIRLINE#1562 IN (AA,AS,DL,UA)\n",
      "+- Project [AIRLINE#1562, DIS_AIRTIME#1620]\n",
      "   +- Project [YEAR#1558, MONTH#1559, DAY#1560, DAY_OF_WEEK#1561, AIRLINE#1562, FLIGHT_NUMBER#1563, TAIL_NUMBER#1564, ORIGIN_AIRPORT#1565, DESTINATION_AIRPORT#1566, SCHEDULED_DEPARTURE#1567, DEPARTURE_TIME#1568, DEPARTURE_DELAY#1569, TAXI_OUT#1570, WHEELS_OFF#1571, SCHEDULED_TIME#1572, ELAPSED_TIME#1573, AIR_TIME#1574, DISTANCE#1575, WHEELS_ON#1576, TAXI_IN#1577, SCHEDULED_ARRIVAL#1578, ARRIVAL_TIME#1579, ARRIVAL_DELAY#1580, DIVERTED#1581, ... 8 more fields]\n",
      "      +- Filter MONTH#1559 IN (6,7,8)\n",
      "         +- Relation [YEAR#1558,MONTH#1559,DAY#1560,DAY_OF_WEEK#1561,AIRLINE#1562,FLIGHT_NUMBER#1563,TAIL_NUMBER#1564,ORIGIN_AIRPORT#1565,DESTINATION_AIRPORT#1566,SCHEDULED_DEPARTURE#1567,DEPARTURE_TIME#1568,DEPARTURE_DELAY#1569,TAXI_OUT#1570,WHEELS_OFF#1571,SCHEDULED_TIME#1572,ELAPSED_TIME#1573,AIR_TIME#1574,DISTANCE#1575,WHEELS_ON#1576,TAXI_IN#1577,SCHEDULED_ARRIVAL#1578,ARRIVAL_TIME#1579,ARRIVAL_DELAY#1580,DIVERTED#1581,... 7 more fields] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [AIRLINE#1562, (cast(DISTANCE#1575 as double) / cast(AIR_TIME#1574 as double)) AS DIS_AIRTIME#1620]\n",
      "+- Filter (MONTH#1559 IN (6,7,8) AND AIRLINE#1562 IN (AA,AS,DL,UA))\n",
      "   +- Relation [YEAR#1558,MONTH#1559,DAY#1560,DAY_OF_WEEK#1561,AIRLINE#1562,FLIGHT_NUMBER#1563,TAIL_NUMBER#1564,ORIGIN_AIRPORT#1565,DESTINATION_AIRPORT#1566,SCHEDULED_DEPARTURE#1567,DEPARTURE_TIME#1568,DEPARTURE_DELAY#1569,TAXI_OUT#1570,WHEELS_OFF#1571,SCHEDULED_TIME#1572,ELAPSED_TIME#1573,AIR_TIME#1574,DISTANCE#1575,WHEELS_ON#1576,TAXI_IN#1577,SCHEDULED_ARRIVAL#1578,ARRIVAL_TIME#1579,ARRIVAL_DELAY#1580,DIVERTED#1581,... 7 more fields] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [AIRLINE#1562, (cast(DISTANCE#1575 as double) / cast(AIR_TIME#1574 as double)) AS DIS_AIRTIME#1620]\n",
      "+- *(1) Filter (MONTH#1559 IN (6,7,8) AND AIRLINE#1562 IN (AA,AS,DL,UA))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [MONTH#1559,AIRLINE#1562,AIR_TIME#1574,DISTANCE#1575] Batched: true, DataFilters: [MONTH#1559 IN (6,7,8), AIRLINE#1562 IN (AA,AS,DL,UA)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/B:/OneDrive/Coding/Python/Courses/PySpark/ingenieria-datos-pytho..., PartitionFilters: [], PushedFilters: [In(MONTH, [6,7,8]), In(AIRLINE, [AA,AS,DL,UA])], ReadSchema: struct<MONTH:int,AIRLINE:string,AIR_TIME:int,DISTANCE:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.explain(\n",
    "    extended=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T11:10:35.896451400Z",
     "start_time": "2023-05-18T11:10:35.755397500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
