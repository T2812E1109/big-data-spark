{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformaciones en un RDD\n",
    "\n",
    "Los RDDs son inmutables, y cada operación crea un nuevo RDD.\n",
    "Las dos principales operaciones que se pueden realizar sobre un RDD son:\n",
    "* **Transformaciones**: Las transformaciones cambian los elementos en el RDD (dividir el elemento de entrada, filtrar elementos o realizar cálculos de algún tipo). Varias transformaciones pueden realizarse en una secuencia, sin embargo, no se lleva a cabo ninguna ejecución durante la planificación. Spark agrega las transformaciones al DAG de cálculo (evaluación perezosa).\n",
    "* **Acciones**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Las transformaciones se pueden dividir en 4 categorías:\n",
    "* **Generales**: filter, distinct, union, map, flatMap, ...\n",
    "* **Matemáticas y estadísticas**: mean, sum, max, min, variance, ...\n",
    "* **De conjunto o relacionales**: join, intersection, subtract, distinct, groupByKey, ...\n",
    "* **Basadas en estructuras de datos**: select, withColumn, drop, orderBy, groupBy, ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Función `map` de PySpark\n",
    "La función `map` de PySpark es una operación de transformación que se aplica a un conjunto de datos distribuidos (RDD) en paralelo. Básicamente, la función `map` toma una función como entrada y aplica esta función a cada elemento del RDD, produciendo un nuevo RDD con los resultados.\n",
    "\n",
    "Por ejemplo, si tenemos un RDD de números y queremos duplicar cada número, podemos usar la función `map` junto con una función que multiplique cada número por dos. De esta manera, PySpark aplicará esta función a cada elemento del RDD y devolverá un nuevo RDD con los números duplicados.\n",
    "\n",
    "En resumen, la función `map` de PySpark es una herramienta útil para transformar un RDD en otro RDD con un formato diferente, mediante la aplicación de una función a cada elemento del RDD original."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"PySpark\").getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T13:44:50.374015Z",
     "start_time": "2023-05-11T13:44:29.280126700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "rdd_sub = rdd.map(lambda x: x - 1).collect()\n",
    "rdd_sub.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "rdd_pair = rdd.map(lambda x: x % 2 == 0)\n",
    "rdd_pair.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T13:44:59.922432100Z",
     "start_time": "2023-05-11T13:44:59.908917700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "['JOHN', 'FRED', 'ANNA', 'JAMES', 'JOE', 'JOLINE', 'MARY']"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_names = sc.parallelize([\"John\", \"Fred\", \"Anna\", \"James\", \"Joe\", \"Joline\", \"Mary\"])\n",
    "rdd_names_upper = rdd_names.map(lambda x: x.upper())\n",
    "rdd_names_upper.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:22:05.200391800Z",
     "start_time": "2023-05-11T14:21:56.465654900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['Hello John', 'Hello Fred', 'Hello Anna', 'Hello James']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_hello = rdd_names.map(lambda x: \"Hello \" + x)\n",
    "rdd_hello.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T13:51:39.160404100Z",
     "start_time": "2023-05-11T13:51:30.838550Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Función `flatMap` de PySpark\n",
    "La función flatMap de PySpark es similar a la función map, ya que también se utiliza para transformar un RDD en otro RDD mediante la aplicación de una función a cada elemento del RDD. Sin embargo, la diferencia principal entre map y flatMap radica en el formato de salida del nuevo RDD.\n",
    "\n",
    "Mientras que map aplica la función a cada elemento del RDD y devuelve un nuevo RDD con los resultados de la función, flatMap toma cada elemento del RDD y genera cero o más elementos para el nuevo RDD. En otras palabras, la función de flatMap devuelve una lista de elementos para cada elemento del RDD de entrada, y luego aplana estas listas en un solo RDD.\n",
    "\n",
    "Por ejemplo, si tenemos un RDD de oraciones y queremos dividir cada oración en palabras, podemos usar la función flatMap junto con una función que divide cada oración en una lista de palabras. En este caso, flatMap generará una lista de palabras para cada oración y luego aplana todas estas listas en un solo RDD de palabras.\n",
    "\n",
    "En resumen, la función flatMap de PySpark es una herramienta útil para transformar un RDD en otro RDD mediante la aplicación de una función a cada elemento del RDD, y generando cero o más elementos para el nuevo RDD."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "rdd_squared = rdd.map(lambda x: (x, x ** 2))\n",
    "rdd_squared.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 1, 2, 4, 3, 9, 4, 16, 5, 25]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_squared_flat = rdd.flatMap(lambda x: (x, x ** 2))\n",
    "rdd_squared_flat.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:01:33.450266800Z",
     "start_time": "2023-05-11T14:01:25.105327600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[('john', 'JOHN'), ('fred', 'FRED'), ('anna', 'ANNA'), ('james', 'JAMES')]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_names_lower_upper = rdd_names.map(lambda x: (x.lower(), x.upper()))\n",
    "rdd_names_lower_upper.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:07:09.622586800Z",
     "start_time": "2023-05-11T14:07:01.296954Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "['john', 'JOHN', 'fred', 'FRED', 'anna', 'ANNA', 'james', 'JAMES']"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_names_lower_upper_flat = rdd_names.flatMap(lambda x: (x.lower(), x.upper()))\n",
    "rdd_names_lower_upper_flat.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:07:35.599453700Z",
     "start_time": "2023-05-11T14:07:27.281147300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Función `filter` de PySpark\n",
    "La función filter de PySpark se utiliza para filtrar elementos de un RDD basándose en una condición booleana. Es decir, toma como entrada una función que devuelve un valor booleano para cada elemento del RDD, y devuelve un nuevo RDD que solo contiene los elementos que cumplen la condición.\n",
    "\n",
    "Por ejemplo, si tenemos un RDD de números y queremos filtrar solo los números pares, podemos usar la función filter junto con una función que compruebe si cada número es par o no. En este caso, filter recorrerá cada elemento del RDD y aplicará la función para comprobar si es par o no. Luego, devolverá un nuevo RDD que solo contiene los números pares.\n",
    "\n",
    "En resumen, la función filter de PySpark es una herramienta útil para filtrar elementos de un RDD basándose en una condición booleana. Esta función devuelve un nuevo RDD que solo contiene los elementos que cumplen la condición especificada."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "[2, 4]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_even = rdd.filter(lambda x: x % 2 == 0)\n",
    "rdd_even.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:13:43.895037700Z",
     "start_time": "2023-05-11T14:13:35.360600100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 3, 5]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_odd = rdd.filter(lambda x: x % 2 != 0)\n",
    "rdd_odd.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:15:21.991563200Z",
     "start_time": "2023-05-11T14:15:13.152521800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "['John', 'James']"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just as reminder, this is the original RDD: rdd_names = sc.parallelize([\"John\", \"Fred\", \"Anna\", \"James\", \"Joe\", \"Joline\", \"Mary\"])\n",
    "rdd_names_start_with_j = rdd_names.filter(lambda x: x.lower().startswith(\"j\"))\n",
    "rdd_names_start_with_j.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:17:09.545566Z",
     "start_time": "2023-05-11T14:17:01.356864200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "['John', 'Joe', 'Joline']"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_names_start_with_j_and_contains_o = rdd_names.filter(\n",
    "    lambda x: x.lower().startswith(\"j\") and \"o\" in x.lower()\n",
    ")\n",
    "rdd_names_start_with_j_and_contains_o.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:22:12.934017200Z",
     "start_time": "2023-05-11T14:22:05.208713800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Función `coalesce` de PySpark\n",
    "La función coalesce de PySpark se utiliza para reducir el número de particiones en un RDD. Es decir, toma como entrada el número de particiones que se desean en el nuevo RDD y devuelve un nuevo RDD que tiene ese número de particiones.\n",
    "\n",
    "La función coalesce es útil cuando tenemos un RDD con muchas particiones y queremos reducir el número de particiones para mejorar el rendimiento de ciertas operaciones. Por ejemplo, si tenemos un RDD con 100 particiones y sabemos que solo necesitamos 10 particiones para realizar una operación determinada, podemos usar la función coalesce para reducir el número de particiones.\n",
    "\n",
    "Es importante tener en cuenta que la función coalesce puede provocar una desigualdad en el tamaño de las particiones del nuevo RDD. En otras palabras, es posible que algunas particiones sean mucho más grandes que otras después de la reducción, lo que puede afectar el rendimiento de algunas operaciones.\n",
    "\n",
    "En resumen, la función coalesce de PySpark se utiliza para reducir el número de particiones en un RDD y mejorar el rendimiento de ciertas operaciones."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "12"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just as reminder, this is the original RDD: rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "rdd.getNumPartitions()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:25:01.704516300Z",
     "start_time": "2023-05-11T14:25:01.641567500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_coalesce = rdd.coalesce(5)\n",
    "rdd_coalesce.getNumPartitions()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:25:07.077924900Z",
     "start_time": "2023-05-11T14:25:07.009366700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Funcion `repartition` de PySpark\n",
    "La función repartition toma como entrada el número deseado de particiones y redistribuye los datos del RDD en esas particiones. Si el nuevo número de particiones es mayor que el número actual de particiones, se realizan operaciones de shuffle para redistribuir los datos de manera equilibrada. Por otro lado, si el nuevo número de particiones es menor que el número actual de particiones, la función repartition se comporta igual que la función coalesce.\n",
    "\n",
    "Es importante tener en cuenta que la función repartition es una operación costosa, ya que implica una operación de shuffle que puede ser muy costosa en términos de tiempo y recursos. Por lo tanto, es importante utilizar esta función solo cuando sea necesario y asegurarse de ajustar el número de particiones al tamaño y la naturaleza de los datos del RDD.\n",
    "\n",
    "En resumen, la función repartition de PySpark se utiliza para redistribuir los datos de un RDD en un número diferente de particiones, y puede ser una operación costosa en términos de tiempo y recursos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_coalesce.getNumPartitions()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:37:55.062266500Z",
     "start_time": "2023-05-11T14:37:55.031014700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_repartition = rdd.repartition(10)\n",
    "rdd_repartition.getNumPartitions()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:37:58.151449700Z",
     "start_time": "2023-05-11T14:37:58.055008Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Función `reduceByKey` de PySpark\n",
    "La función reduceByKey de PySpark es una operación de reducción que se utiliza para combinar los valores de un RDD que tienen la misma clave.\n",
    "\n",
    "Por ejemplo, si tenemos un RDD con pares clave-valor (clave, valor) y queremos sumar los valores asociados a cada clave, podemos utilizar la función reduceByKey para realizar esta operación. La función reduceByKey combinará los valores de todos los pares con la misma clave, aplicando una función de reducción que se especifica como argumento, como la suma en este caso.\n",
    "\n",
    "La función reduceByKey devuelve un nuevo RDD con pares clave-valor, donde las claves son las mismas que las del RDD original y los valores son el resultado de aplicar la función de reducción a los valores asociados a cada clave.\n",
    "\n",
    "En resumen, la función reduceByKey de PySpark es una operación de reducción que se utiliza para combinar los valores de un RDD que tienen la misma clave, aplicando una función de reducción que se especifica como argumento."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "[('b', 24), ('a', 20), ('d', 9), ('c', 25)]"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "[('b', 24), ('a', 20), ('d', 9), ('c', 25)]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_base_reduce_by_key = sc.parallelize(\n",
    "    [\n",
    "        (\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4), (\"c\", 5), (\"a\", 6),\n",
    "        (\"b\", 7), (\"c\", 8), (\"d\", 9), (\"a\", 10), (\"b\", 11), (\"c\", 12)\n",
    "    ]\n",
    ")\n",
    "# a = 20, b = 24, c = 25, d = 9\n",
    "rdd_reduced = rdd_base_reduce_by_key.reduceByKey(lambda x, y: x + y)\n",
    "rdd_reduced.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
